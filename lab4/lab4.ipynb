{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from utils import tile_raster_images\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.cmap'] = 'jet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Implement the RBM that uses CD-1 for training. For input data use MNIST numbers. The visible layer must then have 784 elements, and the hidden layer should have 100 elements. Since the values of the input samples (image) are real numbers in the range [0 1], they can be used as $p(v_i=1)$, so for the initial values of the visible layer, sampling should be performed. Set the mini batch size to 100 samples, and the number of epochs to 100.\n",
    "\n",
    "Subtasks:\n",
    "\n",
    "1. Visualize the weights of $W$ obtained by training and try to interpret the weights associated with some hidden neurons.\n",
    "2. Visualize the reconstruction results of the first 20 MNIST samples. Visualize the values of $p(v_i=1)=σ(∑^N_{j=1} w_{ji}h_j+a_i)$ instead of the binary values obtained by sampling.\n",
    "3. Examine the activation frequency of hidden layer elements and visualize the learned weights of $W$ sorted by the frequency\n",
    "4. Skip the initial sampling/binarization based on the real input data, and use the original input data (real numbers from the range [0 1]) as input layer $v$. How different is such RBM from the previous one?\n",
    "5. Increase the number of Gibs sampling in CDs. What are the differences?\n",
    "6. Examine the effects of varying the learning constant.\n",
    "7. Randomly initialize the hidden layer, run a few Gibbs samplings, and visualize the generated visible layer\n",
    "8. Perform above experiments with a smaller and a larger number of hidden neurons. What do you observe about weights and reconstructions?\n",
    "\n",
    "Use the following template with the utility file [utils.py](https://dlunizg.github.io/assets/lab4/utils.py).\n",
    "\n",
    "REMARK: In addition to filling out the missing code, the template should be tailored as needed, and can be customized freely. So please be especially careful with the claims that some of the code is not working for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trainX, trainY, testX, testY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def init_bias(shape):\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def sample_prob(probs):\n",
    "    \"\"\"Sample vector x by probability vector p (x = 1) = probs\"\"\"\n",
    "    return tf.to_float(tf.random_uniform(tf.shape(probs)) <= probs)\n",
    "\n",
    "\n",
    "def draw_weights(W, shape, N, stat_shape, interpolation=\"bilinear\"):\n",
    "    \"\"\"Visualization of weight\n",
    "     W - weight vector\n",
    "     shape - tuple dimensions for 2D weight display - usually input image dimensions, eg (28,28)\n",
    "     N - number weight vectors\n",
    "     shape_state - Dimension for 2D state display (eg for 100 states (10,10)\n",
    "    \"\"\"\n",
    "    image = (tile_raster_images(\n",
    "        X=W.T,\n",
    "        img_shape=shape,\n",
    "        tile_shape=(int(math.ceil(N/stat_shape[0])), stat_shape[0]),\n",
    "        tile_spacing=(1, 1)))\n",
    "    plt.figure(figsize=(10, 14))\n",
    "    plt.imshow(image, interpolation=interpolation)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "def draw_reconstructions(ins, outs, states, shape_in, shape_state, N):\n",
    "    \"\"\"Visualization of inputs and associated reconstructions and hidden layer states\n",
    "     ins -- input vectors\n",
    "     outs - reconstructed vectors\n",
    "     states - hidden layer state vectors\n",
    "     shape_in - dimension of input images eg (28,28)\n",
    "     shape_state - Dimension for 2D state display (eg for 100 states (10,10)\n",
    "     N - number of samples\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, int(2 * N)))\n",
    "    for i in range(N):\n",
    "        plt.subplot(N, 4, 4*i + 1)\n",
    "        plt.imshow(ins[i].reshape(shape_in), vmin=0, vmax=1, interpolation=\"nearest\")\n",
    "        plt.title(\"Test input\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(N, 4, 4*i + 2)\n",
    "        plt.imshow(outs[i][0:784].reshape(shape_in), vmin=0, vmax=1, interpolation=\"nearest\")\n",
    "        plt.title(\"Reconstruction\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(N, 4, 4*i + 3)\n",
    "        plt.imshow(states[i].reshape(shape_state), vmin=0, vmax=1, interpolation=\"nearest\")\n",
    "        plt.title(\"States\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "def draw_generated(stin, stout, gen, shape_gen, shape_state, N):\n",
    "    \"\"\"Visualization of initial hidden states, final hidden states and associated reconstructions\n",
    "     stin - the initial hidden layer\n",
    "     stout - reconstructed vectors\n",
    "     gen - vector of hidden layer state\n",
    "     shape_gen - dimensional input image eg (28,28)\n",
    "     shape_state - Dimension for 2D state display (eg for 100 states (10,10)\n",
    "     N - number of samples\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, int(2 * N)))\n",
    "    for i in range(N):\n",
    "\n",
    "        plt.subplot(N, 4, 4*i + 1)\n",
    "        plt.imshow(stin[i].reshape(shape_state), vmin=0, vmax=1, interpolation=\"nearest\")\n",
    "        plt.title(\"set state\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(N, 4, 4*i + 2)\n",
    "        plt.imshow(stout[i][0:784].reshape(shape_state), vmin=0, vmax=1, interpolation=\"nearest\")\n",
    "        plt.title(\"final state\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(N, 4, 4*i + 3)\n",
    "        plt.imshow(gen[i].reshape(shape_gen), vmin=0, vmax=1, interpolation=\"nearest\")\n",
    "        plt.title(\"generated visible\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def draw_rec(inp, title, size, Nrows, in_a_row, j):\n",
    "    \"\"\" Draw an iteration of creating the visible layer\n",
    "     inp - visible layer\n",
    "     title - thumbnail title\n",
    "     size - 2D dimensions of visible layer\n",
    "     Nrows - max. number of thumbnail rows \n",
    "     in-a-row. number of thumbnails in one row\n",
    "     j - position of thumbnails in the grid\n",
    "    \"\"\"\n",
    "    plt.subplot(Nrows, in_a_row, j)\n",
    "    plt.imshow(inp.reshape(size), vmin=0, vmax=1, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "def reconstruct(ind, v_shape, h_shape, states, orig, weights, biases):\n",
    "    \"\"\" Sequential visualization of  the visible layer reconstruction\n",
    "     ind - index of digits in orig (matrix with digits as lines)\n",
    "     states - state vectors of input vectors\n",
    "     orig - original input vectors\n",
    "     weights - weight matrix\n",
    "    \"\"\"\n",
    "    j = 1\n",
    "    in_a_row = 6\n",
    "    Nimg = states.shape[1] + 3\n",
    "    Nrows = int(np.ceil(float(Nimg+2)/in_a_row))\n",
    "    \n",
    "    plt.figure(figsize=(12, 2*Nrows))\n",
    "       \n",
    "    draw_rec(states[ind], 'states', h_shape, Nrows, in_a_row, j)\n",
    "    j += 1\n",
    "    draw_rec(orig[ind], 'input', v_shape, Nrows, in_a_row, j)\n",
    "    \n",
    "    reconstr = biases.copy()\n",
    "    j += 1\n",
    "    draw_rec(sigmoid(reconstr), 'biases', v_shape, Nrows, in_a_row, j)\n",
    "    \n",
    "    for i in range(Nh):\n",
    "        if states[ind,i] > 0:\n",
    "            j += 1\n",
    "            reconstr = reconstr + weights[:,i]\n",
    "            titl = '+= s' + str(i+1)\n",
    "            draw_rec(sigmoid(reconstr), titl, v_shape, Nrows, in_a_row, j)\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w_grad(v, h):    \n",
    "    v = tf.reshape(v, [-1, int(v.shape[1]), 1])\n",
    "    h = tf.reshape(h, [-1, 1, int(h.shape[1])])\n",
    "    return tf.reduce_sum(tf.matmul(v, h), reduction_indices=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Task1():\n",
    "    '''\n",
    "    Nh - number of elements of the hidden layer (e.g. 100)\n",
    "    h_shape - shape of hidden layer (e.g. 10x10)\n",
    "    Nv - number of elements of the visible layer (e.g. 784)\n",
    "    v_shape - shape of visible layer (e.g. 28x28)\n",
    "    Nu - number of samples for visualization of reconstruction\n",
    "    gibbs_sampling_steps - number of iterations of Gibbs sampling\n",
    "    alpha - multiplier for weight update\n",
    "    rnd_hidden_init - set hidden unit randomly, not based on visible layer\n",
    "    '''\n",
    "    def __init__(self, Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha, rnd_hidden_init=False):\n",
    "        self.g1 = tf.Graph()\n",
    "        with self.g1.as_default():\n",
    "            \n",
    "            self.v = tf.placeholder(\"float\", [None, Nv]) # visible layer\n",
    "            self.w = init_weights([Nv, Nh]) # weights\n",
    "            self.a = init_bias([Nv]) # bias for visible layer\n",
    "            self.b = init_bias([Nh]) # bias for hidden layer\n",
    "\n",
    "            #https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-i-6df5c4918c15\n",
    "            # p(h) = sigma(b + vw)\n",
    "            self.h_prob = tf.sigmoid(tf.matmul(self.v, self.w) + tf.transpose(self.b))\n",
    "            if rnd_hidden_init:\n",
    "                self.h = tf.to_float(tf.random_uniform(tf.shape(self.h_prob)))\n",
    "            else:\n",
    "                self.h = sample_prob(self.h_prob)\n",
    "                \n",
    "            # Gibbs sampling\n",
    "            self.h_next = self.h\n",
    "            for step in range(gibbs_sampling_steps):\n",
    "                self.gibbs_sample_one()\n",
    "\n",
    "                #Implement the RBM that uses CD-1 for training.\n",
    "                # http://deeplearning.net/tutorial/rbm.html\n",
    "                self.constrastive_divergence()\n",
    "\n",
    "                \n",
    "            self.v_next_prob = tf.sigmoid(tf.matmul(self.h_next, tf.transpose(self.w)) + tf.transpose(self.a))\n",
    "            self.v_next = sample_prob(self.v_next_prob)\n",
    "\n",
    "            err1 = self.v - self.v_next_prob\n",
    "            self.err_sum1 = tf.reduce_mean(err1 * err1)\n",
    "\n",
    "            initialize1 = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "            self.sess1 = tf.Session(graph=self.g1)\n",
    "            self.sess1.run(initialize1)\n",
    "        \n",
    "        \n",
    "    def gibbs_sample_one(self):\n",
    "        '''\n",
    "        Gibbs sampling\n",
    "        -> we take an input vector v_0 and use it to predict the values of the hidden state h_0.\n",
    "        The hidden state are used on the other hand to predict new input state v. This procedure\n",
    "        is repeated k times. This procedure is illustrated in Fig. 2.\n",
    "        @ https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-ii-4b159dce1ffb\n",
    "        '''\n",
    "        self.v_next_prob = tf.sigmoid(tf.matmul(self.h_next, tf.transpose(self.w)) + tf.transpose(self.a))\n",
    "        self.v_next = sample_prob(self.v_next_prob)\n",
    "        self.h_next_prob = tf.sigmoid(tf.matmul(self.v_next, self.w) + tf.transpose(self.b))\n",
    "        self.h_next = sample_prob(self.h_next_prob)\n",
    "        \n",
    "    \n",
    "    def constrastive_divergence(self):\n",
    "        '''\n",
    "        Contrastive Divergence @ towardsdatascience\n",
    "        The update of the weight matrix happens during the Contrastive Divergence step.\n",
    "        Vectors v_0 and v_k are used to calculate the activation probabilities for\n",
    "        hidden values h_0 and h_k (Eq.4).\n",
    "\n",
    "        Using the update matrix the new weights can be calculated with gradient ascent, given by:\n",
    "        delta_w = w_positive_grad - w_negative_grad\n",
    "        w_new = w_old + delta_w*alpha\n",
    "        a_new = a_old + delta_a*alpha\n",
    "        b_new = b_old + delta_b*alpha\n",
    "        '''\n",
    "        w_positive_grad = get_w_grad(self.v, self.h_prob)\n",
    "        w_negative_grad = get_w_grad(self.v_next_prob, self.h_next_prob)\n",
    "\n",
    "        delta_w = (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(self.v)[0]) # .shape[0] - number of rows\n",
    "\n",
    "        update_w = tf.assign_add(self.w, alpha * delta_w)\n",
    "        update_a = tf.assign_add(self.a, alpha * tf.reduce_mean(self.v - self.v_next, 0))\n",
    "        update_b = tf.assign_add(self.b, alpha * tf.reduce_mean(self.h - self.h_next, 0)) \n",
    "        \n",
    "        self.out1 = (update_w, update_a, update_b)\n",
    "        \n",
    "        \n",
    "    def train(self, total_batch):\n",
    "        print('training, total batch:', total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch, label = mnist.train.next_batch(batch_size)\n",
    "            err, _ = self.sess1.run([self.err_sum1, self.out1], feed_dict={self.v: batch})\n",
    "\n",
    "            if i%(int(total_batch/10)) == 0:\n",
    "                print('iter={:<8} error={:2.8f}'.format(i, err))\n",
    "\n",
    "        print('iter={:<8} error={:2.8f}'.format(i, err))\n",
    "        #self.w_s = self.w.eval(session=self.sess1)\n",
    "        #self.a_s = self.a.eval(session=self.sess1)\n",
    "        #self.b_s = self.b.eval(session=self.sess1)\n",
    "        self.w_s, self.a_s, self.b_s = self.sess1.run([self.w, self.a, self.b])\n",
    "        self.vr, self.h_next_s = self.sess1.run([self.v_next_prob, self.h_next], feed_dict={self.v: testX[0:Nu,:]})\n",
    "    \n",
    "    \n",
    "    def get_training_variables(self):\n",
    "        return self.w_s, self.h_next_s, self.vr, self.a_s, self.b_s\n",
    "\n",
    "    \n",
    "    def run_h_next(self, _input):\n",
    "        out_1 = self.sess1.run((self.v_next), feed_dict={self.h: _input})\n",
    "        return out_1\n",
    "\n",
    "    \n",
    "    def run_v_next(self, _input):\n",
    "        out_1_prob, out_1, hout1 = self.sess1.run((self.v_next_prob, self.v_next, self.h_next), feed_dict={self.v: _input})\n",
    "        return out_1_prob, out_1, hout1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialization of class - RBM network\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "n_samples = mnist.train.num_examples\n",
    "total_batch = int(n_samples / batch_size) * epochs\n",
    "\n",
    "\n",
    "Nh = 100 # The number of elements of the hidden layer\n",
    "h_shape = (10,10)\n",
    "Nv = 784 # The number of elements of the visible layer\n",
    "v_shape = (28,28)\n",
    "Nu = 5000 # Number of samples for visualization of reconstruction\n",
    "\n",
    "gibbs_sampling_steps = 1\n",
    "alpha = 0.1\n",
    "\n",
    "rbm = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total batch:', total_batch)\n",
    "total_batch = 100#int(total_batch/55)\n",
    "print('total batch:', total_batch)\n",
    "\n",
    "rbm.train(total_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subtask 1')\n",
    "print('visualization of weights')\n",
    "# visualization of weights\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm.get_training_variables()\n",
    "draw_weights(w_states, v_shape, Nh, h_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subtask 2')\n",
    "print('visualization of reconstructions of 20 MNIST samples')\n",
    "# visualization of reconstructions and states\n",
    "draw_reconstructions(testX, v_reconstructions, h_next_states, v_shape, h_shape, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualization of a reconstructions with the gradual addition of the contributions of active hidden elements\n",
    "reconstruct(0, v_shape, h_shape, h_next_states, testX, w_states, a_states) # the first argument is the digit index in the digit matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The probability that the hidden state is included through Nu input samples\n",
    "'''\n",
    "plt.figure()\n",
    "tmp = (h1s.sum(0)/h1s.shape[0]).reshape(h1_shape)\n",
    "plt.imshow(tmp, vmin=0, vmax=1, interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.title('likelihood of the activation of certain neurons of the hidden layer')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subtask 3')\n",
    "print('Visualization of weights sorted by frequency')\n",
    "# Visualization of weights sorted by frequency\n",
    "tmp = (h_next_states.sum(0)/h_next_states.shape[0]).reshape(h_shape)\n",
    "tmp_ind = (-tmp).argsort(None)\n",
    "draw_weights(w_states[:, tmp_ind], v_shape, Nh, h_shape)\n",
    "plt.title('Sorted weight matrices - from most to the least used')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subtask 4')\n",
    "print('Generating samples from random vectors')\n",
    "# Generating samples from random vectors\n",
    "r_input = np.random.rand(100, Nh)\n",
    "r_input[r_input > 0.9] = 1 # percentage of active - vary freely\n",
    "r_input[r_input < 1] = 0\n",
    "r_input = r_input * 20 # Boosting in case a small percentage is active\n",
    "\n",
    "s = 10\n",
    "for i in range(10):\n",
    "    r_input[i,:] = 0\n",
    "    r_input[i,i]= s\n",
    "\n",
    "out_1 = rbm.run_h_next(r_input)\n",
    "#out_1_prob, out_1, hout1 = sess1.run((v1_prob, v1, h1), feed_dict={h0: r_input})\n",
    "#out_1 = sess1.run((v1), feed_dict={h0: r_input})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#draw_generated(r_input, hout1, out_1_prob, v_shape, h1_shape, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subtask 5')\n",
    "print('Emulation of additional Gibbs sampling using feed_dict')\n",
    "# Emulation of additional Gibbs sampling using feed_dict\n",
    "for i in range(1000):\n",
    "    out_1_prob, out_1, hout1 = rbm.run_v_next(out_1)\n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "\n",
    "draw_generated(r_input, hout1, out_1_prob, v_shape, h_shape, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subtask 5')\n",
    "print('Increase the number of Gibs sampling')\n",
    "\n",
    "total_batch = 100\n",
    "alpha = 0.1\n",
    "\n",
    "gibbs_sampling_steps = 1\n",
    "print('\\ngibbs_sampling_steps {}'.format(gibbs_sampling_steps))\n",
    "rbm = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha)\n",
    "rbm.train(total_batch)\n",
    "\n",
    "gibbs_sampling_steps = 2\n",
    "print('\\ngibbs_sampling_steps {}'.format(gibbs_sampling_steps))\n",
    "rbm = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha)\n",
    "rbm.train(total_batch)\n",
    "\n",
    "gibbs_sampling_steps = 3\n",
    "print('\\ngibbs_sampling_steps {}'.format(gibbs_sampling_steps))\n",
    "rbm = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha)\n",
    "rbm.train(total_batch)\n",
    "\n",
    "gibbs_sampling_steps = 4\n",
    "print('\\ngibbs_sampling_steps {}'.format(gibbs_sampling_steps))\n",
    "rbm = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha)\n",
    "rbm.train(total_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subtask 6')\n",
    "print('different learning steps')\n",
    "\n",
    "gibbs_sampling_steps = 1\n",
    "total_batch = 100\n",
    "\n",
    "alpha = 0.01\n",
    "print('\\nalpha {}'.format(alpha))\n",
    "rbm2 = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha)\n",
    "rbm2.train(total_batch)\n",
    "\n",
    "alpha = 0.1\n",
    "print('\\nalpha {}'.format(alpha))\n",
    "rbm2 = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha)\n",
    "rbm2.train(total_batch)\n",
    "\n",
    "alpha = 0.2\n",
    "print('\\nalpha {}'.format(alpha))\n",
    "rbm2 = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha)\n",
    "rbm2.train(total_batch)\n",
    "\n",
    "alpha = 0.4\n",
    "print('\\nalpha {}'.format(alpha))\n",
    "rbm2 = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha)\n",
    "rbm.train(total_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Subtask 7')\n",
    "print('Randomly initialize the hidden layer, run a few Gibbs samplings, and visualize the generated visible layer')\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "n_samples = mnist.train.num_examples\n",
    "total_batch = int(n_samples / batch_size) * epochs\n",
    "\n",
    "Nh = 100 # The number of elements of the first hidden layer\n",
    "h_shape = (10,10)\n",
    "Nv = 784 # The number of elements of the first visible layer\n",
    "v_shape = (28,28)\n",
    "Nu = 5000 # Number of samples for visualization of reconstruction\n",
    "\n",
    "gibbs_sampling_steps = 1\n",
    "alpha = 0.1\n",
    "\n",
    "total_batch = 100 #int(total_batch/55)\n",
    "\n",
    "print('\\nrandom hidden initialization=False')\n",
    "rbm = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha, rnd_hidden_init=True)\n",
    "rbm.train(total_batch)\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm.get_training_variables()\n",
    "draw_weights(w_states, v_shape, Nh, h_shape)\n",
    "plt.title('random hidden initialization=False')\n",
    "\n",
    "print('\\nrandom hidden initialization=True')\n",
    "rbm = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha, rnd_hidden_init=False)\n",
    "rbm.train(total_batch)\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm.get_training_variables()\n",
    "draw_weights(w_states, v_shape, Nh, h_shape)\n",
    "plt.title('random hidden initialization=True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subtask 8')\n",
    "print('Perform above experiments with a smaller and a larger number of hidden neurons.')\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "n_samples = mnist.train.num_examples\n",
    "total_batch = int(n_samples / batch_size) * epochs\n",
    "\n",
    "\n",
    "print('25 (5x5) neurons in hidden layer')\n",
    "\n",
    "Nh = 25 # The number of elements of the hidden layer\n",
    "h_shape = (5, 5)\n",
    "Nv = 784 # The number of elements of the visible layer\n",
    "v_shape = (28,28)\n",
    "Nu = 5000 # Number of samples for visualization of reconstruction\n",
    "\n",
    "gibbs_sampling_steps = 1\n",
    "alpha = 0.1\n",
    "\n",
    "total_batch = 100 #int(total_batch/55)\n",
    "\n",
    "rbm = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha)\n",
    "rbm.train(total_batch)\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm.get_training_variables()\n",
    "draw_reconstructions(testX, v_reconstructions, h_next_states, v_shape, h_shape, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('400 (20x20) neurons in hidden layer')\n",
    "\n",
    "Nh = 400 # The number of elements of the hidden layer\n",
    "h_shape = (20, 20)\n",
    "Nv = 784 # The number of elements of the visible layer\n",
    "v_shape = (28,28)\n",
    "Nu = 5000 # Number of samples for visualization of reconstruction\n",
    "\n",
    "gibbs_sampling_steps = 1\n",
    "alpha = 0.1\n",
    "\n",
    "total_batch = 100 #int(total_batch/55)\n",
    "\n",
    "rbm = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha)\n",
    "rbm.train(total_batch)\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm.get_training_variables()\n",
    "draw_reconstructions(testX, v_reconstructions, h_next_states, v_shape, h_shape, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Deep Belief Network (DBN) is a deep network that is obtained by stacking multiple RBMs one to another, each of which is trained greedily with inputs from the hidden (“outgoing”) layer of the previous RBM (except the first RBM being trained directly with input samples). In theory, such DBN should increase $p(v)$ which is our initial goal. The use of DBN, ie reconstruction of the input sample, is carried out according to the scheme below. In the upward pass, hidden layers are determined from the visible layer until the highest RBM is reached, then the CD algorithm is executed, then in the downward direction, the lower hidden layers are determined until the visible layer. The weights between the individual layers are the same in the upward and in the downward pass. Implement a three-layer DBN that consists of two greedy RBMs. The first RBM should be as in 1st task, and the second RBM should have a hidden layer of 100 elements.\n",
    "\n",
    "Subtasks:\n",
    "\n",
    "1. Visualize the weights of $W1$ and $W2$ obtained by training.\n",
    "2. Visualize the results of the reconstruction of the first 20 MNIST samples.\n",
    "3. Randomly initialize the topmost hidden layer, run a few Gibbs samplings, and visualize generated visible layer patterns - compare with the previous task.\n",
    "4. Set the number of hidden layer elements of the upper RBM to the number of lower RBM visible layer elements, and set the initial weights $W_2$ to $W_1^T$. What are the effects of change? Visualize elements of the topmost layer as 28x28 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Task2():\n",
    "    '''\n",
    "    Nh - number of elements of the hidden layer (e.g. 100)\n",
    "    h_shape - shape of hidden layer (e.g. 10x10)\n",
    "    Nv - number of elements of the visible layer (e.g. 784)\n",
    "    v_shape - shape of visible layer (e.g. 28x28)\n",
    "    Nu - number of samples for visualization of reconstruction\n",
    "    gibbs_sampling_steps - number of iterations of Gibbs sampling\n",
    "    alpha - multiplier for weight update\n",
    "    w_prev - weights from previous RBN\n",
    "    a_prev - bias for visible layer from previous RBN\n",
    "    b_prev - bias for hidden layer from previous RBN\n",
    "    rnd_hidden_init - set hidden unit randomly, not based on visible layer\n",
    "    '''\n",
    "    def __init__(self, Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha, w_prev, a_prev, b_prev, rnd_hidden_init=False):\n",
    "        self.g2 = tf.Graph()\n",
    "        with self.g2.as_default():\n",
    "            \n",
    "            self.v = tf.placeholder(\"float\", [None, Nv]) # visible layer\n",
    "            self.w = init_weights([Nv, Nh])\n",
    "            self.a = init_bias([Nv])\n",
    "            self.b = init_bias([Nh])\n",
    "            self.w_prev = tf.Variable(w_prev)\n",
    "            self.a_prev = tf.Variable(a_prev)\n",
    "            self.b_prev = tf.Variable(b_prev)\n",
    "\n",
    "            #https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-i-6df5c4918c15\n",
    "            # p(h) = sigma(b + vw)\n",
    "            self.h_prob = tf.sigmoid(tf.matmul(self.v, self.w) + tf.transpose(self.b))\n",
    "            if rnd_hidden_init:\n",
    "                self.h = tf.to_float(tf.random_uniform(tf.shape(self.h_prob)))\n",
    "            else:\n",
    "                self.h = sample_prob(self.h_prob)\n",
    "                \n",
    "            # Gibbs sampling\n",
    "            self.h_next = self.h\n",
    "            for step in range(gibbs_sampling_steps):\n",
    "                self.gibbs_sample_one()\n",
    "\n",
    "                #Implement the RBM that uses CD-1 for training.\n",
    "                # http://deeplearning.net/tutorial/rbm.html\n",
    "                self.constrastive_divergence()\n",
    "\n",
    "                \n",
    "            self.v_next_prob = tf.sigmoid(tf.matmul(self.h_next, tf.transpose(self.w)) + tf.transpose(self.a))\n",
    "            self.v_next = sample_prob(self.v_next_prob)\n",
    "            \n",
    "            err2 = self.v - self.v_next_prob\n",
    "            self.err_sum2 = tf.reduce_mean(err2 * err2)\n",
    "\n",
    "            initialize2 = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "            self.sess2 = tf.Session(graph=self.g2)\n",
    "            self.sess2.run(initialize2)\n",
    "        \n",
    "        \n",
    "    def gibbs_sample_one(self):\n",
    "        '''\n",
    "        Gibbs sampling\n",
    "        -> we take an input vector v_0 and use it to predict the values of the hidden state h_0.\n",
    "        The hidden state are used on the other hand to predict new input state v. This procedure\n",
    "        is repeated k times. This procedure is illustrated in Fig. 2.\n",
    "        @ https://towardsdatascience.com/deep-learning-meets-physics-restricted-boltzmann-machines-part-ii-4b159dce1ffb\n",
    "        '''\n",
    "        self.v_next_prob = tf.sigmoid(tf.matmul(self.h_next, tf.transpose(self.w)) + tf.transpose(self.a))\n",
    "        self.v_next = sample_prob(self.v_next_prob)\n",
    "        self.h_next_prob = tf.sigmoid(tf.matmul(self.v_next, self.w) + tf.transpose(self.b))\n",
    "        self.h_next = sample_prob(self.h_next_prob)\n",
    "        \n",
    "    \n",
    "    def constrastive_divergence(self):\n",
    "        '''\n",
    "        Contrastive Divergence @ towardsdatascience\n",
    "        The update of the weight matrix happens during the Contrastive Divergence step.\n",
    "        Vectors v_0 and v_k are used to calculate the activation probabilities for\n",
    "        hidden values h_0 and h_k (Eq.4).\n",
    "\n",
    "        Using the update matrix the new weights can be calculated with gradient ascent, given by:\n",
    "        delta_w = w_positive_grad - w_negative_grad\n",
    "        w_new = w_old + delta_w*alpha\n",
    "        a_new = a_old + delta_a*alpha\n",
    "        b_new = b_old + delta_b*alpha\n",
    "        '''\n",
    "        w_positive_grad = get_w_grad(self.v, self.h_prob)\n",
    "        w_negative_grad = get_w_grad(self.v_next_prob, self.h_next_prob)\n",
    "\n",
    "        delta_w = (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(self.v)[0]) # .shape[0] - number of rows\n",
    "\n",
    "        update_w = tf.assign_add(self.w, alpha * delta_w)\n",
    "        update_a = tf.assign_add(self.a, alpha * tf.reduce_mean(self.v - self.v_next, 0))\n",
    "        update_b = tf.assign_add(self.b, alpha * tf.reduce_mean(self.h - self.h_next, 0)) \n",
    "        \n",
    "        self.out2 = (update_w, update_a, update_b)\n",
    "        \n",
    "        \n",
    "    def train(self, total_batch):\n",
    "        print('training, total batch:', total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch, label = mnist.train.next_batch(batch_size)\n",
    "            err, _ = self.sess2.run([self.err_sum2, self.out2], feed_dict={self.v: batch})\n",
    "\n",
    "            if i%(int(total_batch/10)) == 0:\n",
    "                print('iter={:<8} error={:2.8f}'.format(i, err))\n",
    "\n",
    "        print('iter={:<8} error={:2.8f}'.format(i, err))\n",
    "        #self.w_s = self.w.eval(session=self.sess2)\n",
    "        #self.a_s = self.a.eval(session=self.sess2)\n",
    "        #self.b_s = self.b.eval(session=self.sess2)\n",
    "        self.w_s, self.a_s, self.b_s = self.sess2.run([self.w, self.a, self.b])\n",
    "        self.vr, self.h_next_s = self.sess2.run([self.v_next_prob, self.h_next], feed_dict={self.v: testX[0:Nu,:]})\n",
    "    \n",
    "    \n",
    "    def get_training_variables(self):\n",
    "        return self.w_s, self.h_next_s, self.vr, self.a_s, self.b_s\n",
    "\n",
    "    \n",
    "    def run_h_next(self, _input):\n",
    "        out_2 = self.sess2.run((self.v_next), feed_dict={self.h: _input})\n",
    "        return out_2\n",
    "\n",
    "    \n",
    "    def run_v_next(self, _input):\n",
    "        out_2_prob, out_2, hout2 = self.sess2.run((self.v_next_prob, self.v_next, self.h_next), feed_dict={self.v: _input})\n",
    "        return out_2_prob, out_2, hout2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of class - RBM network\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "Nh = 100 # The number of elements of the hidden layer\n",
    "h_shape = (10,10)\n",
    "Nv = 784 # The number of elements of the visible layer\n",
    "v_shape = (28,28)\n",
    "Nu = 5000 # Number of samples for visualization of reconstruction\n",
    "\n",
    "gibbs_sampling_steps = 1\n",
    "alpha = 0.1\n",
    "\n",
    "total_batch = 100\n",
    "\n",
    "\n",
    "print('First layer of RBM')\n",
    "rbm1 = Task1(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha)\n",
    "rbm1.train(total_batch)\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm1.get_training_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Second layer of RBM')\n",
    "rbm2 = Task2(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha, w_states, a_states, b_states)\n",
    "rbm2.train(total_batch)\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm2.get_training_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subtask 1')\n",
    "print('Visualize the weights of W1 and W2 obtained by training')\n",
    "\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm1.get_training_variables()\n",
    "draw_weights(w_states, v_shape, Nh, h_shape)\n",
    "plt.title('Weights - first RBM')\n",
    "\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm2.get_training_variables()\n",
    "draw_weights(w_states, v_shape, Nh, h_shape)\n",
    "plt.title('Weights - second RBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subtask 2')\n",
    "print('Visualize the results of the reconstruction of the first 20 MNIST samples')\n",
    "\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm1.get_training_variables()\n",
    "draw_reconstructions(testX, v_reconstructions, h_next_states, v_shape, h_shape, 20)\n",
    "plt.title('Reconstructions after first RBM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm2.get_training_variables()\n",
    "draw_reconstructions(testX, v_reconstructions, h_next_states, v_shape, h_shape, 20)\n",
    "plt.title('Reconstructions after second RBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subtask 3')\n",
    "print('Randomly initialize the topmost hidden layer')\n",
    "\n",
    "total_batch = 100\n",
    "\n",
    "rbm2 = Task2(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha, w_states, a_states, b_states, rnd_hidden_init=True)\n",
    "rbm2.train(total_batch)\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm2.get_training_variables()\n",
    "draw_weights(w_states, v_shape, Nh, h_shape)\n",
    "plt.title('Weights - second RBM with random hidden initialization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subtask 4')\n",
    "print('Set the number of hidden layer elements of the upper RBM to the number of lower RBM visible layer elements,')\n",
    "\n",
    "total_batch = 100\n",
    "Nh = 784#100 # The number of elements of the hidden layer\n",
    "h_shape = (28,28)#(10,10)\n",
    "\n",
    "rbm2 = Task2(Nh, h_shape, Nv, v_shape, Nu, gibbs_sampling_steps, alpha, w_states, a_states, b_states, rnd_hidden_init=True)\n",
    "rbm2.train(total_batch)\n",
    "w_states, h_next_states, v_reconstructions, a_states, b_states = rbm2.get_training_variables()\n",
    "draw_weights(w_states, v_shape, Nh, h_shape)\n",
    "plt.title('Weights - second RBM with hidden size = visible size of first RBM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
