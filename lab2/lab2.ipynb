{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: backprop through FC, ReLU and log softmax\n",
    "\n",
    "Complete implementations of the fully connected layer, hinge nonlinearity layer and the cross entropy loss layer (`F`C, `ReLU`, `SoftmaxCrossEntropyWithLogits`). The cross entropy loss layer calculates divergence between the exact distribution and the distribution predicted by the model:\n",
    "\n",
    "> equation\n",
    "\n",
    "C stands for the number of classes, $x$ is the input to the softmax function (which may be called classification score or logit), $y$ is the groundtruth distribution of the example over all classes (often given as a one-hot vector), while $softmax_i (x)$ is the softmax output for the class $i$. For simplicity, we have shown the loss for only one example while in practice it will often be average loss over the whole batch. To perform backprop, we first need to calculate the gradient of this function with respect to input $\\frac{∂L}{∂x}$. The derivation of this gradient may be simplified by plugging-in the definition of the softmax:\n",
    "\n",
    "> equation\n",
    "\n",
    "Now we can determine the derivative of the goal function with respect to single classification score ($x_k$):\n",
    "\n",
    "> equation\n",
    "\n",
    "Finally, we obtain the gradient with respect to layer inputs as the difference between the model distribution and the exact distribution:\n",
    "\n",
    "> equation\n",
    "\n",
    "In order to setup your environment, set the appropriate paths in variables `DATA_DIR` and` SAVE_DIR` and compile the cython module `im2col_cython.pyx` with `python3 setup_cython.py build_ext --inplace`.\n",
    "\n",
    "Test implementations of your layers by invoking the script `check_grads.py`. A satisfactory error should be less than $10^{−5}$\n",
    "\n",
    "if you use double precision. Finally, start the training of a convolutional model by invoking the script `train.py`. The script will download the MNIST dataset to the `SAVE_DIR` directory.\n",
    "\n",
    "During training you can monitor the visualization of the learned filters which are saved in the `SAVE_DIR` directory. Since each weight correspond to a single image pixel, we recommend to turn off antialiasing for best viewing experience. We recommend to use geeqie on Linux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: regularization\n",
    "\n",
    "In this task you need to add support for L2 parameter regularization. Complete the implementation of the `L2Regularizer` layer and train the regularized model by invoking `train_l2reg.py`. Examine effects of the regularization hyper-parameter by training three different models with $λ=1e−3, λ=1e−2, λ=1e−1$\n",
    "and comparing the filters from the first layer and the achieved accuracy.\n",
    "\n",
    "> Random initializations of the filters from the first layer (top) and the trained filters (bottom) with lambda = 0.01.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Tensorflow MNIST\n",
    "\n",
    "Define and train a tensorflow model which is equivalent to the regularized model from Task 2. Define an identical architecture and training parameters in order to reproduce the results. Use the convolution operations from `tf.nn.conv2d` or `tf.contrib.layers.convolution2d`. Study the official documentation for the [convolution](https://www.tensorflow.org/versions/master/api_docs/python/nn.html#convolution) suport in Tensorflow. Visualize the trained filters from the first layer during training, as in Task 2.\n",
    "\n",
    "An example of using convolutions defined in the `tf.contrib` package is shown below. If you prefer to use `tf.nn.conv2d`, please consult the official [tutorial](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#build-a-multilayer-convolutional-network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "def build_model(inputs, labels, num_classes):\n",
    "    weight_decay = ...\n",
    "    conv1sz = ...\n",
    "    fc3sz = ...\n",
    "    with tf.contrib.framework.arg_scope([layers.convolution2d],\n",
    "            kernel_size=5, stride=1, padding='SAME', activation_fn=tf.nn.relu,\n",
    "            weights_initializer=layers.variance_scaling_initializer(),\n",
    "            weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "\n",
    "        net = layers.convolution2d(inputs, conv1sz, scope='conv1')\n",
    "        # ostatak konvolucijskih i pooling slojeva\n",
    "        ...\n",
    "\n",
    "    with tf.contrib.framework.arg_scope([layers.fully_connected],\n",
    "            activation_fn=tf.nn.relu,\n",
    "            weights_initializer=layers.variance_scaling_initializer(),\n",
    "            weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "\n",
    "        # sada definiramo potpuno povezane slojeve\n",
    "        # ali najprije prebacimo 4D tenzor u matricu\n",
    "        net = layers.flatten(inputs)\n",
    "        net = layers.fully_connected(net, fc3sz, scope='fc3')\n",
    "\n",
    "    logits = layers.fully_connected(net, num_classes, activation_fn=None, scope='logits')\n",
    "    loss = ...\n",
    "\n",
    "    return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Tensorflow CIFAR 10\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset contains 50000 images for training and validation, and 10000 test images. The images have dimensions 32x32 and they belong to 10 classes. Download the dataset version for Python [here](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz). Use the following code to load and prepair the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def shuffle_data(data_x, data_y):\n",
    "    indices = np.arange(data_x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_data_x = np.ascontiguousarray(data_x[indices])\n",
    "    shuffled_data_y = np.ascontiguousarray(data_y[indices])\n",
    "    return shuffled_data_x, shuffled_data_y\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "DATA_DIR = '/path/to/data/'\n",
    "\n",
    "\n",
    "train_x = np.ndarray((0, img_height * img_width * num_channels), dtype=np.float32)\n",
    "train_y = []\n",
    "for i in range(1, 6):\n",
    "    subset = unpickle(os.path.join(DATA_DIR, 'data_batch_%d' % i))\n",
    "    train_x = np.vstack((train_x, subset['data']))\n",
    "    train_y += subset['labels']\n",
    "train_x = train_x.reshape((-1, num_channels, img_height, img_width)).transpose(0,2,3,1)\n",
    "train_y = np.array(train_y, dtype=np.int32)\n",
    "\n",
    "subset = unpickle(os.path.join(DATA_DIR, 'test_batch'))\n",
    "test_x = subset['data'].reshape((-1, num_channels, img_height, img_width)).transpose(0,2,3,1).astype(np.float32)\n",
    "test_y = np.array(subset['labels'], dtype=np.int32)\n",
    "\n",
    "valid_size = 5000\n",
    "train_x, train_y = shuffle_data(train_x, train_y)\n",
    "valid_x = train_x[:valid_size, ...]\n",
    "valid_y = train_y[:valid_size, ...]\n",
    "train_x = train_x[valid_size:, ...]\n",
    "train_y = train_y[valid_size:, ...]\n",
    "data_mean = train_x.mean((0,1,2))\n",
    "data_std = train_x.std((0,1,2))\n",
    "\n",
    "train_x = (train_x - data_mean) / data_std\n",
    "valid_x = (valid_x - data_mean) / data_std\n",
    "test_x = (test_x - data_mean) / data_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to train a convolutional model in Tensorflow. We propose a simple model which should yield about 70% accuracy in image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv(16,5) -> relu() -> pool(3,2) -> conv(32,5) -> relu() -> pool(3,2) -> fc(256) -> relu() -> fc(128) -> relu() -> fc(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `conv(16,5)` represents a convolution with 16 feature maps and filter dimensions 5x5, `pool(3,2)` is a max-pooling layer operating on patches 3x3 and the stride 2.\n",
    "\n",
    "Write the function `evaluate` which compares the predicted and correct class indices and determines the following classification performance indicators: overall classification accuracy, confusion matrix, as well as precision and recall for particular classes. In the implementation, first determine the confusion matrix, and then use it to determine all other indicators. During training, invoke `evaluate` after each epoch both on the training and on the validation dataset, and graph the average loss, the training rate and overall classification accuracy. We recommend that function receives the data, the correct class indices, and the required tensorflow operations. Be careful not to invoke the training operation. The function should output the recovered indicators to the console.\n",
    "\n",
    "> A typical loss graph when the training proceeds well.\n",
    "\n",
    "Visualize random initializations and the trained filters from the first layer. You can access the variable which holds the weight of the first layer by invoking the `tf.contrib.framework.get_variables` method with the scope in which the variable is used in the model. We supply an example of how that might look like below. The scope will depend on the code which you actually used while defining the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "conv1_var = tf.contrib.framework.get_variables('model/conv1_1/weights:0')[0]\n",
    "conv1_weights = conv1_var.eval(session=sess)\n",
    "draw_conv_filters(0, 0, conv1_weights, SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide code which you can use for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_conv_filters(epoch, step, weights, save_dir):\n",
    "    w = weights.copy()\n",
    "    num_filters = w.shape[3]\n",
    "    num_channels = w.shape[2]\n",
    "    k = w.shape[0]\n",
    "    assert w.shape[0] == w.shape[1]\n",
    "    w = w.reshape(k, k, num_channels, num_filters)\n",
    "    w -= w.min()\n",
    "    w /= w.max()\n",
    "    border = 1\n",
    "    cols = 8\n",
    "    rows = math.ceil(num_filters / cols)\n",
    "    width = cols * k + (cols-1) * border\n",
    "    height = rows * k + (rows-1) * border\n",
    "    img = np.zeros([height, width, num_channels])\n",
    "    for i in range(num_filters):\n",
    "        r = int(i / cols) * (k + border)\n",
    "        c = int(i % cols) * (k + border)\n",
    "        img[r:r+k,c:c+k,:] = w[:,:,:,i]\n",
    "    filename = 'epoch_%02d_step_%06d.png' % (epoch, step)\n",
    "    ski.io.imsave(os.path.join(save_dir, filename), img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CIFAR-10: random initializations (top) and the learned filters in the first layer (bottom) with regularization lambda = 0.0001.\n",
    "    \n",
    "\n",
    "Visualize 20 incorrectly classified images with the largest loss and output the correct class and the top 3 predicted classes. Pay attention that in order to visualize image, you first need to undo the normalization of the mean value and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skimage as ski\n",
    "import skimage.io\n",
    "\n",
    "def draw_image(img, mean, std):\n",
    "    img *= std\n",
    "    img += mean\n",
    "    img = img.astype(np.uint8)\n",
    "    ski.io.imshow(img)\n",
    "    ski.io.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the code for producing graphs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(save_dir, data):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16,8))\n",
    "\n",
    "    linewidth = 2\n",
    "    legend_size = 10\n",
    "    train_color = 'm'\n",
    "    val_color = 'c'\n",
    "\n",
    "    num_points = len(data['train_loss'])\n",
    "    x_data = np.linspace(1, num_points, num_points)\n",
    "    ax1.set_title('Cross-entropy loss')\n",
    "    ax1.plot(x_data, data['train_loss'], marker='o', color=train_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='train')\n",
    "    ax1.plot(x_data, data['valid_loss'], marker='o', color=val_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='validation')\n",
    "    ax1.legend(loc='upper right', fontsize=legend_size)\n",
    "    ax2.set_title('Average class accuracy')\n",
    "    ax2.plot(x_data, data['train_acc'], marker='o', color=train_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='train')\n",
    "    ax2.plot(x_data, data['valid_acc'], marker='o', color=val_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='validation')\n",
    "    ax2.legend(loc='upper left', fontsize=legend_size)\n",
    "    ax3.set_title('Learning rate')\n",
    "    ax3.plot(x_data, data['lr'], marker='o', color=train_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='learning_rate')\n",
    "    ax3.legend(loc='upper left', fontsize=legend_size)\n",
    "\n",
    "    save_path = os.path.join(save_dir, 'training_plot.pdf')\n",
    "    print('Plotting in: ', save_path)\n",
    "    plt.savefig(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_data = {}\n",
    "plot_data['train_loss'] = []\n",
    "plot_data['valid_loss'] = []\n",
    "plot_data['train_acc'] = []\n",
    "plot_data['valid_acc'] = []\n",
    "plot_data['lr'] = []\n",
    "for epoch_num in range(1, num_epochs + 1):\n",
    "    train_x, train_y = shuffle_data(train_x, train_y)\n",
    "    for step in range(num_batches):\n",
    "        offset = step * batch_size \n",
    "        # s ovim kodom pazite da je broj primjera djeljiv s batch_size\n",
    "        batch_x = train_x[offset:(offset + batch_size), ...]\n",
    "        batch_y = train_y[offset:(offset + batch_size)]\n",
    "        feed_dict = {node_x: batch_x, node_y: batch_y}\n",
    "        start_time = time.time()\n",
    "        run_ops = [train_op, loss, logits]\n",
    "        ret_val = sess.run(run_ops, feed_dict=feed_dict)\n",
    "        _, loss_val, logits_val = ret_val\n",
    "        duration = time.time() - start_time\n",
    "        if (step+1) % 50 == 0:\n",
    "            sec_per_batch = float(duration)\n",
    "            format_str = 'epoch %d, step %d / %d, loss = %.2f (%.3f sec/batch)'\n",
    "            print(format_str % (epoch_num, step+1, num_batches, loss_val, sec_per_batch))\n",
    "\n",
    "    print('Train error:')\n",
    "    train_loss, train_acc = evaluate(logits, loss, train_x, train_y)\n",
    "    print('Validation error:')\n",
    "    valid_loss, valid_acc = evaluate(logits, loss, valid_x, valid_y)\n",
    "    plot_data['train_loss'] += [train_loss]\n",
    "    plot_data['valid_loss'] += [valid_loss]\n",
    "    plot_data['train_acc'] += [train_acc]\n",
    "    plot_data['valid_acc'] += [valid_acc]\n",
    "    plot_data['lr'] += [lr.eval(session=sess)]\n",
    "    plot_training_progress(SAVE_DIR, plot_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have access to a GPU, you might want to try obtaining better results with a more powerful model. In that case, [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130) you can find a review of state-of-the-art results on this dataset. As you see, best approaches achieve around 96% overall classification accuracy. Two important tricks to achieve this are image upsampling and jittering. Image upsampling ensures that early convolutions detect very low level features, while jittering prevents overfitting. Without these techniques, it will be very hard for you to achieve more than 90% overall classification accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
