{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage as ski\n",
    "import skimage.io\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import nn\n",
    "import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Tensorflow MNIST\n",
    "\n",
    "Define and train a tensorflow model which is equivalent to the regularized model from Task 2. Define an identical architecture and training parameters in order to reproduce the results. Use the convolution operations from `tf.nn.conv2d` or `tf.contrib.layers.convolution2d`. Study the official documentation for the [convolution](https://www.tensorflow.org/versions/master/api_docs/python/nn.html#convolution) suport in Tensorflow. Visualize the trained filters from the first layer during training, as in Task 2.\n",
    "\n",
    "An example of using convolutions defined in the `tf.contrib` package is shown below. If you prefer to use `tf.nn.conv2d`, please consult the official [tutorial](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#build-a-multilayer-convolutional-network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "def build_model(inputs, labels, num_classes):\n",
    "    weight_decay = ...\n",
    "    conv1sz = ...\n",
    "    fc3sz = ...\n",
    "    with tf.contrib.framework.arg_scope([layers.convolution2d],\n",
    "            kernel_size=5, stride=1, padding='SAME', activation_fn=tf.nn.relu,\n",
    "            weights_initializer=layers.variance_scaling_initializer(),\n",
    "            weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "\n",
    "        net = layers.convolution2d(inputs, conv1sz, scope='conv1')\n",
    "        # ostatak konvolucijskih i pooling slojeva\n",
    "        ...\n",
    "\n",
    "    with tf.contrib.framework.arg_scope([layers.fully_connected],\n",
    "            activation_fn=tf.nn.relu,\n",
    "            weights_initializer=layers.variance_scaling_initializer(),\n",
    "            weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "\n",
    "        # sada definiramo potpuno povezane slojeve\n",
    "        # ali najprije prebacimo 4D tenzor u matricu\n",
    "        net = layers.flatten(inputs)\n",
    "        net = layers.fully_connected(net, fc3sz, scope='fc3')\n",
    "\n",
    "    logits = layers.fully_connected(net, num_classes, activation_fn=None, scope='logits')\n",
    "    loss = ...\n",
    "\n",
    "    return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'MNIST original'\n",
    "SAVE_DIR = 'task3_out'\n",
    "import os\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "config = {}\n",
    "config['max_epochs'] = 8\n",
    "config['batch_size'] = 50\n",
    "config['save_dir'] = SAVE_DIR\n",
    "config['weight_decay'] = 1e-3\n",
    "config['lr_policy'] = {1:{'lr':1e-1}, 3:{'lr':1e-2}, 5:{'lr':1e-3}, 7:{'lr':1e-4}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-178604a4b213>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST original\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST original\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST original\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST original\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(100) \n",
    "np.random.seed(int(time.time() * 1e6) % 2**31)\n",
    "dataset = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "train_x = dataset.train.images\n",
    "train_x = train_x.reshape([-1, 28, 28, 1])\n",
    "train_y = dataset.train.labels\n",
    "\n",
    "valid_x = dataset.validation.images\n",
    "valid_x = valid_x.reshape([-1, 28, 28, 1])\n",
    "valid_y = dataset.validation.labels\n",
    "\n",
    "test_x = dataset.test.images\n",
    "test_x = test_x.reshape([-1, 28, 28, 1])\n",
    "test_y = dataset.test.labels\n",
    "\n",
    "train_mean = train_x.mean()\n",
    "train_x -= train_mean\n",
    "valid_x -= train_mean\n",
    "test_x -= train_mean\n",
    "\n",
    "weight_decay = config['weight_decay']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42286426/what-is-the-difference-between-xavier-initializer-and-xavier-initializer-conv2d\n",
    "# itializer is designed to keep the scale of the gradients roughly the same in all layers\n",
    "from tensorflow.contrib.layers import xavier_initializer_conv2d\n",
    "from tensorflow.contrib.layers import xavier_initializer\n",
    "\n",
    "\n",
    "##\n",
    "# Dimensions of the tensor is: [batch, height, width, channels]\n",
    "##\n",
    "\n",
    "\n",
    "def Convolution(_input, w, b, n_strides=1):\n",
    "    # strides determines how much the window shifts by in each of the dimensions.\n",
    "    # The typical use sets the first (the batch) and last (the depth) stride to 1.\n",
    "    node = tf.nn.conv2d(_input, w, strides=[1, n_strides, n_strides, 1], padding='SAME')\n",
    "    node = tf.nn.bias_add(node, b)\n",
    "    return node\n",
    "\n",
    "\n",
    "def ReLU(_input):\n",
    "    return tf.nn.relu(_input)\n",
    "\n",
    "\n",
    "def MaxPooling(_input, ks=2, n_strides=2):\n",
    "    # https://stackoverflow.com/questions/38601452/the-usages-of-ksize-in-tf-nn-max-pool\n",
    "    # ksize - kernel size. eg. [1,2,2,1] - kernel 2x2\n",
    "    \n",
    "    # strides - determines how much the window shifts by in each of the dimensions.\n",
    "    # The typical use sets the first (the batch) and last (the depth) stride to 1.\n",
    "    \n",
    "    # https://stackoverflow.com/questions/34642595/tensorflow-strides-argument\n",
    "    # The input to the convolution has shape=[1, 32, 32, 1].\n",
    "    # If you specify strides=[1,1,1,1] with padding=SAME, then the output of the filter will be [1, 32, 32, 8].\n",
    "    return tf.nn.max_pool(_input, ksize=[1, ks, ks, 1], strides=[1, n_strides, n_strides, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def Flatten(_input, w):\n",
    "    # Given tensor, this operation returns a tensor that has the same values as tensor with shape shape.\n",
    "    # this is needed tbecause fully connected layer is simple \"1-d vector\"\n",
    "    return tf.reshape(_input, [-1, w.shape[0]])\n",
    "\n",
    "\n",
    "def FC(_input, w, b):\n",
    "    return tf.matmul(_input, w) + b\n",
    "\n",
    "\n",
    "def SoftmaxCrossEntropyWithLogits(logits, labels):\n",
    "    # softmax_cross_entropy_with_logits_v2 is new version of deprecated softmax_cross_entropy_with_logits\n",
    "    # https://stats.stackexchange.com/questions/327348/how-is-softmax-cross-entropy-with-logits-different-from-softmax-cross-entropy-wi\n",
    "    # \"in supervised learning one doesn't need to backpropagate to labels\"\n",
    "    # thats why tf.stop_gradient in softmax_cross_entropy_with_logits_v2\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=tf.stop_gradient(labels)))\n",
    "\n",
    "\n",
    "def RegularizedLoss(data_loss, regularizers):\n",
    "    return data_loss + weight_decay*regularizers\n",
    "\n",
    "\n",
    "def L2Regularizer(weights):\n",
    "    return tf.nn.l2_loss(weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = dataset.train.labels.shape[1] # [0] - num of labels, [1] - num of classes\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Yoh_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "net=X\n",
    "regularizers=0\n",
    "#net = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "\n",
    "w_conv1 = tf.get_variable('w_conv1', [5, 5, 1, 16], initializer=xavier_initializer_conv2d())\n",
    "b_conv1 = tf.Variable(tf.zeros([16]), name='b_conv1')\n",
    "net = Convolution(net, w_conv1, b_conv1)\n",
    "regularizers += L2Regularizer(w_conv1)\n",
    "net = MaxPooling(net)\n",
    "net = ReLU(net)\n",
    "\n",
    "w_conv2 = tf.get_variable('w_conv2', [5, 5, 16, 32], initializer=xavier_initializer_conv2d())\n",
    "b_conv2 = tf.Variable(tf.zeros([32]), name='b_conv2')\n",
    "net = Convolution(net, w_conv2, b_conv2)\n",
    "regularizers += L2Regularizer(w_conv2)\n",
    "net = MaxPooling(net)\n",
    "net = ReLU(net)\n",
    "\n",
    "## 7x7\n",
    "w_fc3 = tf.get_variable('w_fc3', [7*7*32, 512], initializer=xavier_initializer())\n",
    "b_fc3 = tf.Variable(tf.zeros([512]), name='b_fc3')\n",
    "net = Flatten(net, w_fc3)\n",
    "net = FC(net, w_fc3, b_fc3)\n",
    "regularizers += L2Regularizer(w_fc3)\n",
    "net = ReLU(net)\n",
    "\n",
    "w_fc4 = tf.get_variable('w_fc4', [512, n_classes], initializer=xavier_initializer())\n",
    "b_fc4 = tf.Variable(tf.zeros([n_classes]), name='b_fc4')\n",
    "net = FC(net, w_fc4, b_fc4)\n",
    "\n",
    "data_loss = SoftmaxCrossEntropyWithLogits(net, Yoh_)\n",
    "loss = RegularizedLoss(data_loss, regularizers)\n",
    "\n",
    "lr = tf.placeholder(tf.float32)\n",
    "train_step =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(session, train_x, train_y, valid_x, valid_y, config, logits):\n",
    "    session.run(tf.initialize_all_variables())\n",
    "\n",
    "    lr_policy = config['lr_policy']\n",
    "    batch_size = config['batch_size']\n",
    "    max_epochs = config['max_epochs']\n",
    "    save_dir = config['save_dir']\n",
    "    num_examples = train_x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        if epoch in lr_policy:\n",
    "            solver_config = lr_policy[epoch]\n",
    "        cnt_correct = 0\n",
    "\n",
    "        permutation_idx = np.random.permutation(num_examples)\n",
    "        train_x = train_x[permutation_idx]\n",
    "        train_y = train_y[permutation_idx]\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            # store mini-batch to ndarray\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size, :]\n",
    "            batch_y = train_y[i*batch_size:(i+1)*batch_size, :]\n",
    "\n",
    "            data_dict = {X: batch_x, Yoh_: batch_y, lr:solver_config['lr']}\n",
    "            ##logits = forward_pass(net, batch_x)\n",
    "            #logits_val = session.run(logits, feed_dict=data_dict)\n",
    "            ##loss_val = loss.forward(logits, batch_y)\n",
    "            #loss_val = session.run(loss, feed_dict=data_dict)\n",
    "            #session.run(train_step, feed_dict=data_dict)\n",
    "            # optimization - one execution of session.run\n",
    "            logits_val, loss_val, _ = session.run([logits, loss, train_step], feed_dict=data_dict)\n",
    "\n",
    "            # compute classification accuracy\n",
    "            yp = np.argmax(logits_val, 1)\n",
    "            yt = np.argmax(batch_y, 1)\n",
    "            cnt_correct += (yp == yt).sum()\n",
    "            ##grads = backward_pass(net, loss, logits, batch_y)\n",
    "            ##sgd_update_params(grads, solver_config)\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                print(\"epoch %d, step %d/%d, batch loss = %.2f\" % (epoch, i*batch_size, num_examples, loss_val))\n",
    "            if i % 100 == 0:\n",
    "                w = session.run(w_conv1)\n",
    "                draw_conv_filters(epoch, i*batch_size, \"conv1\", w, save_dir)\n",
    "            if i > 0 and i % 50 == 0:\n",
    "                print(\"Train accuracy = %.2f\" % (cnt_correct / ((i+1)*batch_size) * 100))\n",
    "                \n",
    "        print(\"Train accuracy = %.2f\" % (cnt_correct / num_examples * 100))\n",
    "        evaluate(session, \"Validation\", valid_x, valid_y, config, logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(session, name, x, y, config, logits):\n",
    "    print(\"\\nRunning evaluation: \", name)\n",
    "    batch_size = config['batch_size']\n",
    "    num_examples = x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    cnt_correct = 0\n",
    "    loss_avg = 0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_x = x[i*batch_size:(i+1)*batch_size, :]\n",
    "        batch_y = y[i*batch_size:(i+1)*batch_size, :]\n",
    "        \n",
    "        ##logits = forward_pass(net, batch_x)\n",
    "        data_dict = {X: batch_x, Yoh_: batch_y}\n",
    "        logits_val = session.run(logits, feed_dict=data_dict)\n",
    "        \n",
    "        yp = np.argmax(logits_val, 1)\n",
    "        yt = np.argmax(batch_y, 1)\n",
    "        cnt_correct += (yp == yt).sum()\n",
    "        loss_val = session.run(loss, feed_dict=data_dict)\n",
    "        loss_avg += loss_val\n",
    "        #print(\"step %d / %d, loss = %.2f\" % (i*batch_size, num_examples, loss_val / batch_size))\n",
    "    valid_acc = cnt_correct / num_examples * 100\n",
    "    loss_avg /= num_batches\n",
    "    print(name + \" accuracy = %.2f\" % valid_acc)\n",
    "    print(name + \" avg loss = %.2f\\n\" % loss_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_conv_filters(epoch, step, name, weights, save_dir):\n",
    "    k, k, C, num_filters = weights.shape\n",
    "\n",
    "    w = weights.copy().swapaxes(0, 3).swapaxes(1,2)\n",
    "    w = w.reshape(num_filters, C, k, k)\n",
    "    w -= w.min()\n",
    "    w /= w.max()\n",
    "\n",
    "    border = 1\n",
    "    cols = 8\n",
    "    rows = math.ceil(num_filters / cols)\n",
    "    width = cols * k + (cols-1) * border\n",
    "    height = rows * k + (rows-1) * border\n",
    "    #for i in range(C):\n",
    "    for i in range(1):\n",
    "        img = np.zeros([height, width])\n",
    "        for j in range(num_filters):\n",
    "            r = int(j / cols) * (k + border)\n",
    "            c = int(j % cols) * (k + border)\n",
    "            img[r:r+k,c:c+k] = w[j,i]\n",
    "        filename = '%s_epoch_%02d_step_%06d_input_%03d.png' % (name, epoch, step, i)\n",
    "        ski.io.imsave(os.path.join(save_dir, filename), img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 0/55000, batch loss = 2.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 250/55000, batch loss = 2.59\n",
      "epoch 1, step 500/55000, batch loss = 2.45\n",
      "epoch 1, step 750/55000, batch loss = 2.28\n",
      "epoch 1, step 1000/55000, batch loss = 2.41\n",
      "epoch 1, step 1250/55000, batch loss = 1.49\n",
      "epoch 1, step 1500/55000, batch loss = 1.12\n",
      "epoch 1, step 1750/55000, batch loss = 1.45\n",
      "epoch 1, step 2000/55000, batch loss = 0.84\n",
      "epoch 1, step 2250/55000, batch loss = 1.00\n",
      "epoch 1, step 2500/55000, batch loss = 0.81\n",
      "Train accuracy = 55.80\n",
      "epoch 1, step 2750/55000, batch loss = 0.66\n",
      "epoch 1, step 3000/55000, batch loss = 0.71\n",
      "epoch 1, step 3250/55000, batch loss = 0.82\n",
      "epoch 1, step 3500/55000, batch loss = 0.56\n",
      "epoch 1, step 3750/55000, batch loss = 0.80\n",
      "epoch 1, step 4000/55000, batch loss = 0.77\n",
      "epoch 1, step 4250/55000, batch loss = 0.74\n",
      "epoch 1, step 4500/55000, batch loss = 0.65\n",
      "epoch 1, step 4750/55000, batch loss = 0.90\n",
      "epoch 1, step 5000/55000, batch loss = 0.64\n",
      "Train accuracy = 71.88\n",
      "epoch 1, step 5250/55000, batch loss = 0.66\n",
      "epoch 1, step 5500/55000, batch loss = 0.72\n",
      "epoch 1, step 5750/55000, batch loss = 0.70\n",
      "epoch 1, step 6000/55000, batch loss = 0.67\n",
      "epoch 1, step 6250/55000, batch loss = 0.74\n",
      "epoch 1, step 6500/55000, batch loss = 0.63\n",
      "epoch 1, step 6750/55000, batch loss = 0.63\n",
      "epoch 1, step 7000/55000, batch loss = 0.58\n",
      "epoch 1, step 7250/55000, batch loss = 0.53\n",
      "epoch 1, step 7500/55000, batch loss = 0.57\n",
      "Train accuracy = 78.81\n",
      "epoch 1, step 7750/55000, batch loss = 0.53\n",
      "epoch 1, step 8000/55000, batch loss = 0.54\n",
      "epoch 1, step 8250/55000, batch loss = 0.75\n",
      "epoch 1, step 8500/55000, batch loss = 0.54\n",
      "epoch 1, step 8750/55000, batch loss = 0.53\n",
      "epoch 1, step 9000/55000, batch loss = 0.50\n",
      "epoch 1, step 9250/55000, batch loss = 0.62\n",
      "epoch 1, step 9500/55000, batch loss = 0.49\n",
      "epoch 1, step 9750/55000, batch loss = 0.55\n",
      "epoch 1, step 10000/55000, batch loss = 0.53\n",
      "Train accuracy = 82.80\n",
      "epoch 1, step 10250/55000, batch loss = 0.77\n",
      "epoch 1, step 10500/55000, batch loss = 0.43\n",
      "epoch 1, step 10750/55000, batch loss = 0.57\n",
      "epoch 1, step 11000/55000, batch loss = 0.53\n",
      "epoch 1, step 11250/55000, batch loss = 0.73\n",
      "epoch 1, step 11500/55000, batch loss = 0.68\n",
      "epoch 1, step 11750/55000, batch loss = 0.53\n",
      "epoch 1, step 12000/55000, batch loss = 0.64\n",
      "epoch 1, step 12250/55000, batch loss = 0.79\n",
      "epoch 1, step 12500/55000, batch loss = 0.47\n",
      "Train accuracy = 85.24\n",
      "epoch 1, step 12750/55000, batch loss = 0.47\n",
      "epoch 1, step 13000/55000, batch loss = 0.48\n",
      "epoch 1, step 13250/55000, batch loss = 0.56\n",
      "epoch 1, step 13500/55000, batch loss = 0.45\n",
      "epoch 1, step 13750/55000, batch loss = 0.48\n",
      "epoch 1, step 14000/55000, batch loss = 0.64\n",
      "epoch 1, step 14250/55000, batch loss = 0.71\n",
      "epoch 1, step 14500/55000, batch loss = 0.48\n",
      "epoch 1, step 14750/55000, batch loss = 0.48\n",
      "epoch 1, step 15000/55000, batch loss = 0.53\n",
      "Train accuracy = 86.96\n",
      "epoch 1, step 15250/55000, batch loss = 0.51\n",
      "epoch 1, step 15500/55000, batch loss = 0.42\n",
      "epoch 1, step 15750/55000, batch loss = 0.52\n",
      "epoch 1, step 16000/55000, batch loss = 0.45\n",
      "epoch 1, step 16250/55000, batch loss = 0.48\n",
      "epoch 1, step 16500/55000, batch loss = 0.61\n",
      "epoch 1, step 16750/55000, batch loss = 0.42\n",
      "epoch 1, step 17000/55000, batch loss = 0.50\n",
      "epoch 1, step 17250/55000, batch loss = 0.45\n",
      "epoch 1, step 17500/55000, batch loss = 0.40\n",
      "Train accuracy = 88.30\n",
      "epoch 1, step 17750/55000, batch loss = 0.43\n",
      "epoch 1, step 18000/55000, batch loss = 0.49\n",
      "epoch 1, step 18250/55000, batch loss = 0.50\n",
      "epoch 1, step 18500/55000, batch loss = 0.70\n",
      "epoch 1, step 18750/55000, batch loss = 0.54\n",
      "epoch 1, step 19000/55000, batch loss = 0.48\n",
      "epoch 1, step 19250/55000, batch loss = 0.42\n",
      "epoch 1, step 19500/55000, batch loss = 0.48\n",
      "epoch 1, step 19750/55000, batch loss = 0.47\n",
      "epoch 1, step 20000/55000, batch loss = 0.52\n",
      "Train accuracy = 89.35\n",
      "epoch 1, step 20250/55000, batch loss = 0.46\n",
      "epoch 1, step 20500/55000, batch loss = 0.70\n",
      "epoch 1, step 20750/55000, batch loss = 0.59\n",
      "epoch 1, step 21000/55000, batch loss = 0.55\n",
      "epoch 1, step 21250/55000, batch loss = 0.45\n",
      "epoch 1, step 21500/55000, batch loss = 0.50\n",
      "epoch 1, step 21750/55000, batch loss = 0.51\n",
      "epoch 1, step 22000/55000, batch loss = 0.44\n",
      "epoch 1, step 22250/55000, batch loss = 0.49\n",
      "epoch 1, step 22500/55000, batch loss = 0.41\n",
      "Train accuracy = 90.09\n",
      "epoch 1, step 22750/55000, batch loss = 0.46\n",
      "epoch 1, step 23000/55000, batch loss = 0.54\n",
      "epoch 1, step 23250/55000, batch loss = 0.55\n",
      "epoch 1, step 23500/55000, batch loss = 0.49\n",
      "epoch 1, step 23750/55000, batch loss = 0.46\n",
      "epoch 1, step 24000/55000, batch loss = 0.59\n",
      "epoch 1, step 24250/55000, batch loss = 0.39\n",
      "epoch 1, step 24500/55000, batch loss = 0.53\n",
      "epoch 1, step 24750/55000, batch loss = 0.53\n",
      "epoch 1, step 25000/55000, batch loss = 0.45\n",
      "Train accuracy = 90.73\n",
      "epoch 1, step 25250/55000, batch loss = 0.52\n",
      "epoch 1, step 25500/55000, batch loss = 0.41\n",
      "epoch 1, step 25750/55000, batch loss = 0.43\n",
      "epoch 1, step 26000/55000, batch loss = 0.50\n",
      "epoch 1, step 26250/55000, batch loss = 0.69\n",
      "epoch 1, step 26500/55000, batch loss = 0.50\n",
      "epoch 1, step 26750/55000, batch loss = 0.49\n",
      "epoch 1, step 27000/55000, batch loss = 0.44\n",
      "epoch 1, step 27250/55000, batch loss = 0.43\n",
      "epoch 1, step 27500/55000, batch loss = 0.49\n",
      "Train accuracy = 91.26\n",
      "epoch 1, step 27750/55000, batch loss = 0.39\n",
      "epoch 1, step 28000/55000, batch loss = 0.57\n",
      "epoch 1, step 28250/55000, batch loss = 0.38\n",
      "epoch 1, step 28500/55000, batch loss = 0.64\n",
      "epoch 1, step 28750/55000, batch loss = 0.46\n",
      "epoch 1, step 29000/55000, batch loss = 0.50\n",
      "epoch 1, step 29250/55000, batch loss = 0.43\n",
      "epoch 1, step 29500/55000, batch loss = 0.45\n",
      "epoch 1, step 29750/55000, batch loss = 0.45\n",
      "epoch 1, step 30000/55000, batch loss = 0.43\n",
      "Train accuracy = 91.73\n",
      "epoch 1, step 30250/55000, batch loss = 0.43\n",
      "epoch 1, step 30500/55000, batch loss = 0.39\n",
      "epoch 1, step 30750/55000, batch loss = 0.48\n",
      "epoch 1, step 31000/55000, batch loss = 0.47\n",
      "epoch 1, step 31250/55000, batch loss = 0.41\n",
      "epoch 1, step 31500/55000, batch loss = 0.71\n",
      "epoch 1, step 31750/55000, batch loss = 0.46\n",
      "epoch 1, step 32000/55000, batch loss = 0.53\n",
      "epoch 1, step 32250/55000, batch loss = 0.45\n",
      "epoch 1, step 32500/55000, batch loss = 0.43\n",
      "Train accuracy = 92.15\n",
      "epoch 1, step 32750/55000, batch loss = 0.50\n",
      "epoch 1, step 33000/55000, batch loss = 0.38\n",
      "epoch 1, step 33250/55000, batch loss = 0.42\n",
      "epoch 1, step 33500/55000, batch loss = 0.38\n",
      "epoch 1, step 33750/55000, batch loss = 0.39\n",
      "epoch 1, step 34000/55000, batch loss = 0.40\n",
      "epoch 1, step 34250/55000, batch loss = 0.50\n",
      "epoch 1, step 34500/55000, batch loss = 0.52\n",
      "epoch 1, step 34750/55000, batch loss = 0.42\n",
      "epoch 1, step 35000/55000, batch loss = 0.49\n",
      "Train accuracy = 92.48\n",
      "epoch 1, step 35250/55000, batch loss = 0.51\n",
      "epoch 1, step 35500/55000, batch loss = 0.41\n",
      "epoch 1, step 35750/55000, batch loss = 0.40\n",
      "epoch 1, step 36000/55000, batch loss = 0.38\n",
      "epoch 1, step 36250/55000, batch loss = 0.53\n",
      "epoch 1, step 36500/55000, batch loss = 0.42\n",
      "epoch 1, step 36750/55000, batch loss = 0.48\n",
      "epoch 1, step 37000/55000, batch loss = 0.47\n",
      "epoch 1, step 37250/55000, batch loss = 0.52\n",
      "epoch 1, step 37500/55000, batch loss = 0.40\n",
      "Train accuracy = 92.81\n",
      "epoch 1, step 37750/55000, batch loss = 0.39\n",
      "epoch 1, step 38000/55000, batch loss = 0.44\n",
      "epoch 1, step 38250/55000, batch loss = 0.55\n",
      "epoch 1, step 38500/55000, batch loss = 0.40\n",
      "epoch 1, step 38750/55000, batch loss = 0.38\n",
      "epoch 1, step 39000/55000, batch loss = 0.41\n",
      "epoch 1, step 39250/55000, batch loss = 0.42\n",
      "epoch 1, step 39500/55000, batch loss = 0.41\n",
      "epoch 1, step 39750/55000, batch loss = 0.39\n",
      "epoch 1, step 40000/55000, batch loss = 0.39\n",
      "Train accuracy = 93.07\n",
      "epoch 1, step 40250/55000, batch loss = 0.48\n",
      "epoch 1, step 40500/55000, batch loss = 0.54\n",
      "epoch 1, step 40750/55000, batch loss = 0.42\n",
      "epoch 1, step 41000/55000, batch loss = 0.39\n",
      "epoch 1, step 41250/55000, batch loss = 0.38\n",
      "epoch 1, step 41500/55000, batch loss = 0.39\n",
      "epoch 1, step 41750/55000, batch loss = 0.42\n",
      "epoch 1, step 42000/55000, batch loss = 0.43\n",
      "epoch 1, step 42250/55000, batch loss = 0.42\n",
      "epoch 1, step 42500/55000, batch loss = 0.39\n",
      "Train accuracy = 93.37\n",
      "epoch 1, step 42750/55000, batch loss = 0.43\n",
      "epoch 1, step 43000/55000, batch loss = 0.37\n",
      "epoch 1, step 43250/55000, batch loss = 0.36\n",
      "epoch 1, step 43500/55000, batch loss = 0.39\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-240-18410d0957aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Test\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-237-9b087466ca22>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(session, train_x, train_y, valid_x, valid_y, config, logits)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m#loss_val = session.run(loss, feed_dict=data_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m#session.run(train_step, feed_dict=data_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mlogits_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;31m# compute classification accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\software\\anaconda3_5.0.0_3.6_x64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "train(session, train_x, train_y, valid_x, valid_y, config, net)\n",
    "evaluate(session, \"Test\", test_x, test_y, config, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Tensorflow CIFAR 10\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset contains 50000 images for training and validation, and 10000 test images. The images have dimensions 32x32 and they belong to 10 classes. Download the dataset version for Python [here](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz). Use the following code to load and prepair the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def shuffle_data(data_x, data_y):\n",
    "    indices = np.arange(data_x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_data_x = np.ascontiguousarray(data_x[indices])\n",
    "    shuffled_data_y = np.ascontiguousarray(data_y[indices])\n",
    "    return shuffled_data_x, shuffled_data_y\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "DATA_DIR = '/path/to/data/'\n",
    "\n",
    "\n",
    "train_x = np.ndarray((0, img_height * img_width * num_channels), dtype=np.float32)\n",
    "train_y = []\n",
    "for i in range(1, 6):\n",
    "    subset = unpickle(os.path.join(DATA_DIR, 'data_batch_%d' % i))\n",
    "    train_x = np.vstack((train_x, subset['data']))\n",
    "    train_y += subset['labels']\n",
    "train_x = train_x.reshape((-1, num_channels, img_height, img_width)).transpose(0,2,3,1)\n",
    "train_y = np.array(train_y, dtype=np.int32)\n",
    "\n",
    "subset = unpickle(os.path.join(DATA_DIR, 'test_batch'))\n",
    "test_x = subset['data'].reshape((-1, num_channels, img_height, img_width)).transpose(0,2,3,1).astype(np.float32)\n",
    "test_y = np.array(subset['labels'], dtype=np.int32)\n",
    "\n",
    "valid_size = 5000\n",
    "train_x, train_y = shuffle_data(train_x, train_y)\n",
    "valid_x = train_x[:valid_size, ...]\n",
    "valid_y = train_y[:valid_size, ...]\n",
    "train_x = train_x[valid_size:, ...]\n",
    "train_y = train_y[valid_size:, ...]\n",
    "data_mean = train_x.mean((0,1,2))\n",
    "data_std = train_x.std((0,1,2))\n",
    "\n",
    "train_x = (train_x - data_mean) / data_std\n",
    "valid_x = (valid_x - data_mean) / data_std\n",
    "test_x = (test_x - data_mean) / data_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to train a convolutional model in Tensorflow. We propose a simple model which should yield about 70% accuracy in image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv(16,5) -> relu() -> pool(3,2) -> conv(32,5) -> relu() -> pool(3,2) -> fc(256) -> relu() -> fc(128) -> relu() -> fc(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `conv(16,5)` represents a convolution with 16 feature maps and filter dimensions 5x5, `pool(3,2)` is a max-pooling layer operating on patches 3x3 and the stride 2.\n",
    "\n",
    "Write the function `evaluate` which compares the predicted and correct class indices and determines the following classification performance indicators: overall classification accuracy, confusion matrix, as well as precision and recall for particular classes. In the implementation, first determine the confusion matrix, and then use it to determine all other indicators. During training, invoke `evaluate` after each epoch both on the training and on the validation dataset, and graph the average loss, the training rate and overall classification accuracy. We recommend that function receives the data, the correct class indices, and the required tensorflow operations. Be careful not to invoke the training operation. The function should output the recovered indicators to the console.\n",
    "\n",
    "> A typical loss graph when the training proceeds well.\n",
    "\n",
    "Visualize random initializations and the trained filters from the first layer. You can access the variable which holds the weight of the first layer by invoking the `tf.contrib.framework.get_variables` method with the scope in which the variable is used in the model. We supply an example of how that might look like below. The scope will depend on the code which you actually used while defining the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "conv1_var = tf.contrib.framework.get_variables('model/conv1_1/weights:0')[0]\n",
    "conv1_weights = conv1_var.eval(session=sess)\n",
    "draw_conv_filters(0, 0, conv1_weights, SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide code which you can use for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_conv_filters(epoch, step, weights, save_dir):\n",
    "    w = weights.copy()\n",
    "    num_filters = w.shape[3]\n",
    "    num_channels = w.shape[2]\n",
    "    k = w.shape[0]\n",
    "    assert w.shape[0] == w.shape[1]\n",
    "    w = w.reshape(k, k, num_channels, num_filters)\n",
    "    w -= w.min()\n",
    "    w /= w.max()\n",
    "    border = 1\n",
    "    cols = 8\n",
    "    rows = math.ceil(num_filters / cols)\n",
    "    width = cols * k + (cols-1) * border\n",
    "    height = rows * k + (rows-1) * border\n",
    "    img = np.zeros([height, width, num_channels])\n",
    "    for i in range(num_filters):\n",
    "        r = int(i / cols) * (k + border)\n",
    "        c = int(i % cols) * (k + border)\n",
    "        img[r:r+k,c:c+k,:] = w[:,:,:,i]\n",
    "    filename = 'epoch_%02d_step_%06d.png' % (epoch, step)\n",
    "    ski.io.imsave(os.path.join(save_dir, filename), img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CIFAR-10: random initializations (top) and the learned filters in the first layer (bottom) with regularization lambda = 0.0001.\n",
    "    \n",
    "\n",
    "Visualize 20 incorrectly classified images with the largest loss and output the correct class and the top 3 predicted classes. Pay attention that in order to visualize image, you first need to undo the normalization of the mean value and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skimage as ski\n",
    "import skimage.io\n",
    "\n",
    "def draw_image(img, mean, std):\n",
    "    img *= std\n",
    "    img += mean\n",
    "    img = img.astype(np.uint8)\n",
    "    ski.io.imshow(img)\n",
    "    ski.io.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the code for producing graphs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(save_dir, data):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16,8))\n",
    "\n",
    "    linewidth = 2\n",
    "    legend_size = 10\n",
    "    train_color = 'm'\n",
    "    val_color = 'c'\n",
    "\n",
    "    num_points = len(data['train_loss'])\n",
    "    x_data = np.linspace(1, num_points, num_points)\n",
    "    ax1.set_title('Cross-entropy loss')\n",
    "    ax1.plot(x_data, data['train_loss'], marker='o', color=train_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='train')\n",
    "    ax1.plot(x_data, data['valid_loss'], marker='o', color=val_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='validation')\n",
    "    ax1.legend(loc='upper right', fontsize=legend_size)\n",
    "    ax2.set_title('Average class accuracy')\n",
    "    ax2.plot(x_data, data['train_acc'], marker='o', color=train_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='train')\n",
    "    ax2.plot(x_data, data['valid_acc'], marker='o', color=val_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='validation')\n",
    "    ax2.legend(loc='upper left', fontsize=legend_size)\n",
    "    ax3.set_title('Learning rate')\n",
    "    ax3.plot(x_data, data['lr'], marker='o', color=train_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='learning_rate')\n",
    "    ax3.legend(loc='upper left', fontsize=legend_size)\n",
    "\n",
    "    save_path = os.path.join(save_dir, 'training_plot.pdf')\n",
    "    print('Plotting in: ', save_path)\n",
    "    plt.savefig(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_data = {}\n",
    "plot_data['train_loss'] = []\n",
    "plot_data['valid_loss'] = []\n",
    "plot_data['train_acc'] = []\n",
    "plot_data['valid_acc'] = []\n",
    "plot_data['lr'] = []\n",
    "for epoch_num in range(1, num_epochs + 1):\n",
    "    train_x, train_y = shuffle_data(train_x, train_y)\n",
    "    for step in range(num_batches):\n",
    "        offset = step * batch_size \n",
    "        # s ovim kodom pazite da je broj primjera djeljiv s batch_size\n",
    "        batch_x = train_x[offset:(offset + batch_size), ...]\n",
    "        batch_y = train_y[offset:(offset + batch_size)]\n",
    "        feed_dict = {node_x: batch_x, node_y: batch_y}\n",
    "        start_time = time.time()\n",
    "        run_ops = [train_op, loss, logits]\n",
    "        ret_val = sess.run(run_ops, feed_dict=feed_dict)\n",
    "        _, loss_val, logits_val = ret_val\n",
    "        duration = time.time() - start_time\n",
    "        if (step+1) % 50 == 0:\n",
    "            sec_per_batch = float(duration)\n",
    "            format_str = 'epoch %d, step %d / %d, loss = %.2f (%.3f sec/batch)'\n",
    "            print(format_str % (epoch_num, step+1, num_batches, loss_val, sec_per_batch))\n",
    "\n",
    "    print('Train error:')\n",
    "    train_loss, train_acc = evaluate(logits, loss, train_x, train_y)\n",
    "    print('Validation error:')\n",
    "    valid_loss, valid_acc = evaluate(logits, loss, valid_x, valid_y)\n",
    "    plot_data['train_loss'] += [train_loss]\n",
    "    plot_data['valid_loss'] += [valid_loss]\n",
    "    plot_data['train_acc'] += [train_acc]\n",
    "    plot_data['valid_acc'] += [valid_acc]\n",
    "    plot_data['lr'] += [lr.eval(session=sess)]\n",
    "    plot_training_progress(SAVE_DIR, plot_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have access to a GPU, you might want to try obtaining better results with a more powerful model. In that case, [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130) you can find a review of state-of-the-art results on this dataset. As you see, best approaches achieve around 96% overall classification accuracy. Two important tricks to achieve this are image upsampling and jittering. Image upsampling ensures that early convolutions detect very low level features, while jittering prevents overfitting. Without these techniques, it will be very hard for you to achieve more than 90% overall classification accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
