{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage as ski\n",
    "import skimage.io\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# https://stackoverflow.com/questions/42286426/what-is-the-difference-between-xavier-initializer-and-xavier-initializer-conv2d\n",
    "# itializer is designed to keep the scale of the gradients roughly the same in all layers\n",
    "from tensorflow.contrib.layers import xavier_initializer_conv2d\n",
    "from tensorflow.contrib.layers import xavier_initializer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Tensorflow MNIST\n",
    "\n",
    "Define and train a tensorflow model which is equivalent to the regularized model from Task 2. Define an identical architecture and training parameters in order to reproduce the results. Use the convolution operations from `tf.nn.conv2d` or `tf.contrib.layers.convolution2d`. Study the official documentation for the [convolution](https://www.tensorflow.org/versions/master/api_docs/python/nn.html#convolution) suport in Tensorflow. Visualize the trained filters from the first layer during training, as in Task 2.\n",
    "\n",
    "An example of using convolutions defined in the `tf.contrib` package is shown below. If you prefer to use `tf.nn.conv2d`, please consult the official [tutorial](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#build-a-multilayer-convolutional-network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### don't run!\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "def build_model(inputs, labels, num_classes):\n",
    "    weight_decay = ...\n",
    "    conv1sz = ...\n",
    "    fc3sz = ...\n",
    "    with tf.contrib.framework.arg_scope([layers.convolution2d],\n",
    "            kernel_size=5, stride=1, padding='SAME', activation_fn=tf.nn.relu,\n",
    "            weights_initializer=layers.variance_scaling_initializer(),\n",
    "            weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "\n",
    "        net = layers.convolution2d(inputs, conv1sz, scope='conv1')\n",
    "        # ostatak konvolucijskih i pooling slojeva\n",
    "        ...\n",
    "\n",
    "    with tf.contrib.framework.arg_scope([layers.fully_connected],\n",
    "            activation_fn=tf.nn.relu,\n",
    "            weights_initializer=layers.variance_scaling_initializer(),\n",
    "            weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "\n",
    "        # sada definiramo potpuno povezane slojeve\n",
    "        # ali najprije prebacimo 4D tenzor u matricu\n",
    "        net = layers.flatten(inputs)\n",
    "        net = layers.fully_connected(net, fc3sz, scope='fc3')\n",
    "\n",
    "    logits = layers.fully_connected(net, num_classes, activation_fn=None, scope='logits')\n",
    "    loss = ...\n",
    "\n",
    "    return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'MNIST original'\n",
    "SAVE_DIR = 'task3_out'\n",
    "import os\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "config = {}\n",
    "config['max_epochs'] = 8\n",
    "config['batch_size'] = 50\n",
    "config['save_dir'] = SAVE_DIR\n",
    "config['weight_decay'] = 1e-3\n",
    "config['lr_policy'] = {1:{'lr':1e-1}, 3:{'lr':1e-2}, 5:{'lr':1e-3}, 7:{'lr':1e-4}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(100) \n",
    "np.random.seed(int(time.time() * 1e6) % 2**31)\n",
    "dataset = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "train_x = dataset.train.images\n",
    "train_x = train_x.reshape([-1, 28, 28, 1])\n",
    "train_y = dataset.train.labels\n",
    "\n",
    "valid_x = dataset.validation.images\n",
    "valid_x = valid_x.reshape([-1, 28, 28, 1])\n",
    "valid_y = dataset.validation.labels\n",
    "\n",
    "test_x = dataset.test.images\n",
    "test_x = test_x.reshape([-1, 28, 28, 1])\n",
    "test_y = dataset.test.labels\n",
    "\n",
    "train_mean = train_x.mean()\n",
    "train_x -= train_mean\n",
    "valid_x -= train_mean\n",
    "test_x -= train_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##\n",
    "# Dimensions of tensor are: [batch, height, width, channels]\n",
    "##\n",
    "\n",
    "\n",
    "def Convolution(_input, w, b, n_strides=1):\n",
    "    # strides determines how much the window shifts by in each of the dimensions.\n",
    "    # The typical use sets the first (the batch) and last (the depth) stride to 1.\n",
    "    node = tf.nn.conv2d(_input, w, strides=[1, n_strides, n_strides, 1], padding='SAME')\n",
    "    node = tf.nn.bias_add(node, b)\n",
    "    return node\n",
    "\n",
    "\n",
    "def ReLU(_input):\n",
    "    return tf.nn.relu(_input)\n",
    "\n",
    "\n",
    "def MaxPooling(_input, ks=2, n_strides=2):\n",
    "    # https://stackoverflow.com/questions/38601452/the-usages-of-ksize-in-tf-nn-max-pool\n",
    "    # ksize - kernel size. eg. [1,2,2,1] - kernel 2x2\n",
    "    \n",
    "    # strides - determines how much the window shifts by in each of the dimensions.\n",
    "    # The typical use sets the first (the batch) and last (the depth) stride to 1.\n",
    "    \n",
    "    # https://stackoverflow.com/questions/34642595/tensorflow-strides-argument\n",
    "    # The input to the convolution has shape=[1, 32, 32, 1].\n",
    "    # If you specify strides=[1,1,1,1] with padding=SAME, then the output of the filter will be [1, 32, 32, 8].\n",
    "    return tf.nn.max_pool(_input, ksize=[1, ks, ks, 1], strides=[1, n_strides, n_strides, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def Flatten(_input, w):\n",
    "    # Given tensor, this operation returns a tensor that has the same values as tensor with shape shape.\n",
    "    # this is needed because fully connected layer is simple \"1-d vector\"\n",
    "    return tf.layers.flatten(_input)\n",
    "\n",
    "\n",
    "def FC(_input, w, b):\n",
    "    return tf.matmul(_input, w) + b\n",
    "\n",
    "\n",
    "def SoftmaxCrossEntropyWithLogits(logits, labels):\n",
    "    # softmax_cross_entropy_with_logits_v2 is new version of deprecated softmax_cross_entropy_with_logits\n",
    "    # https://stats.stackexchange.com/questions/327348/how-is-softmax-cross-entropy-with-logits-different-from-softmax-cross-entropy-wi\n",
    "    # \"in supervised learning one doesn't need to backpropagate to labels\"\n",
    "    # thats why tf.stop_gradient in softmax_cross_entropy_with_logits_v2\n",
    "    return tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=tf.stop_gradient(labels))\n",
    "\n",
    "\n",
    "def ReduceMean(_input):\n",
    "    return tf.reduce_mean(_input)\n",
    "\n",
    "\n",
    "def RegularizedLoss(loss, regularizers):\n",
    "    return loss + config['weight_decay']*regularizers\n",
    "\n",
    "\n",
    "def L2Regularizer(weights):\n",
    "    return tf.nn.l2_loss(weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = dataset.train.labels.shape[1] # [0] - num of labels, [1] - num of classes\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# MNIST: 28x28px images\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Yoh_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "net=X\n",
    "regularizers=0\n",
    "\n",
    "w_conv1 = tf.get_variable('w_conv1', [5, 5, 1, 16], initializer=xavier_initializer_conv2d())\n",
    "b_conv1 = tf.Variable(tf.zeros([16]), name='b_conv1')\n",
    "net = Convolution(net, w_conv1, b_conv1)\n",
    "regularizers += L2Regularizer(w_conv1)\n",
    "net = MaxPooling(net)\n",
    "net = ReLU(net)\n",
    "\n",
    "w_conv2 = tf.get_variable('w_conv2', [5, 5, 16, 32], initializer=xavier_initializer_conv2d())\n",
    "b_conv2 = tf.Variable(tf.zeros([32]), name='b_conv2')\n",
    "net = Convolution(net, w_conv2, b_conv2)\n",
    "regularizers += L2Regularizer(w_conv2)\n",
    "net = MaxPooling(net)\n",
    "net = ReLU(net)\n",
    "\n",
    "## 7x7\n",
    "w_fc3 = tf.get_variable('w_fc3', [7*7*32, 512], initializer=xavier_initializer())\n",
    "b_fc3 = tf.Variable(tf.zeros([512]), name='b_fc3')\n",
    "net = Flatten(net, w_fc3)\n",
    "net = FC(net, w_fc3, b_fc3)\n",
    "regularizers += L2Regularizer(w_fc3)\n",
    "net = ReLU(net)\n",
    "\n",
    "w_fc4 = tf.get_variable('w_fc4', [512, n_classes], initializer=xavier_initializer())\n",
    "b_fc4 = tf.Variable(tf.zeros([n_classes]), name='b_fc4')\n",
    "net = FC(net, w_fc4, b_fc4)\n",
    "\n",
    "loss_per_sample = SoftmaxCrossEntropyWithLogits(net, Yoh_)\n",
    "loss_mean = ReduceMean(loss_per_sample)\n",
    "loss = RegularizedLoss(loss_mean, regularizers)\n",
    "\n",
    "lr = tf.placeholder(tf.float32) # learning rate\n",
    "train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(session, train_x, train_y, valid_x, valid_y, config, logits):\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    lr_policy = config['lr_policy']\n",
    "    batch_size = config['batch_size']\n",
    "    max_epochs = config['max_epochs']\n",
    "    save_dir = config['save_dir']\n",
    "    num_examples = train_x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        if epoch in lr_policy:\n",
    "            solver_config = lr_policy[epoch]\n",
    "        cnt_correct = 0\n",
    "\n",
    "        permutation_idx = np.random.permutation(num_examples)\n",
    "        train_x = train_x[permutation_idx]\n",
    "        train_y = train_y[permutation_idx]\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            # store mini-batch to ndarray\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size, :]\n",
    "            batch_y = train_y[i*batch_size:(i+1)*batch_size, :]\n",
    "\n",
    "            data_dict = {X: batch_x, Yoh_: batch_y, lr:solver_config['lr']}\n",
    "            ##logits = forward_pass(net, batch_x)\n",
    "            #logits_val = session.run(logits, feed_dict=data_dict)\n",
    "            ##loss_val = loss.forward(logits, batch_y)\n",
    "            #loss_val = session.run(loss, feed_dict=data_dict)\n",
    "            #session.run(train_step, feed_dict=data_dict)\n",
    "            # optimization - one execution of session.run\n",
    "            # https://github.com/aymericdamien/TensorFlow-Examples/issues/22#issuecomment-202861887\n",
    "            logits_val, loss_val, _ = session.run([logits, loss, train_step], feed_dict=data_dict)\n",
    "\n",
    "            # compute classification accuracy\n",
    "            yp = np.argmax(logits_val, 1) # predicted classes\n",
    "            yt = np.argmax(batch_y, 1) # true classes\n",
    "            cnt_correct += (yp == yt).sum()\n",
    "            ##grads = backward_pass(net, loss, logits, batch_y)\n",
    "            ##sgd_update_params(grads, solver_config)\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                print(\"epoch %d, step %d/%d, batch loss = %.2f\" % (epoch, i*batch_size, num_examples, loss_val))\n",
    "            if i % 100 == 0:\n",
    "                w = session.run(w_conv1)\n",
    "                draw_conv_filters(epoch, i*batch_size, \"conv1\", w, save_dir)\n",
    "            if i > 0 and i % 50 == 0:\n",
    "                print(\"Train accuracy = %.2f\" % (cnt_correct / ((i+1)*batch_size) * 100))\n",
    "                \n",
    "        print(\"Train accuracy = %.2f\" % (cnt_correct / num_examples * 100))\n",
    "        evaluate(session, \"Validation\", valid_x, valid_y, config, logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(session, name, x, y, config, logits):\n",
    "    print(\"\\nRunning evaluation: \", name)\n",
    "    batch_size = config['batch_size']\n",
    "    num_examples = x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    cnt_correct = 0\n",
    "    loss_avg = 0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_x = x[i*batch_size:(i+1)*batch_size, :]\n",
    "        batch_y = y[i*batch_size:(i+1)*batch_size, :]\n",
    "        \n",
    "        ##logits = forward_pass(net, batch_x)\n",
    "        data_dict = {X: batch_x, Yoh_: batch_y}\n",
    "        logits_val, loss_val = session.run([logits, loss], feed_dict=data_dict)\n",
    "        \n",
    "        yp = np.argmax(logits_val, 1)\n",
    "        yt = np.argmax(batch_y, 1)\n",
    "        cnt_correct += (yp == yt).sum()\n",
    "        #####loss_val = session.run(loss, feed_dict=data_dict) # optimized -> calculated up ^^^\n",
    "        loss_avg += loss_val\n",
    "        #print(\"step %d / %d, loss = %.2f\" % (i*batch_size, num_examples, loss_val / batch_size))\n",
    "    valid_acc = cnt_correct / num_examples * 100\n",
    "    loss_avg /= num_batches\n",
    "    print(name + \" accuracy = %.2f\" % valid_acc)\n",
    "    print(name + \" avg loss = %.2f\\n\" % loss_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_conv_filters(epoch, step, name, weights, save_dir):\n",
    "    k, k, C, num_filters = weights.shape\n",
    "\n",
    "    w = weights.copy().swapaxes(0, 3).swapaxes(1,2)\n",
    "    w = w.reshape(num_filters, C, k, k)\n",
    "    w -= w.min()\n",
    "    w /= w.max()\n",
    "\n",
    "    border = 1\n",
    "    cols = 8\n",
    "    rows = math.ceil(num_filters / cols)\n",
    "    width = cols * k + (cols-1) * border\n",
    "    height = rows * k + (rows-1) * border\n",
    "    #for i in range(C):\n",
    "    for i in range(1):\n",
    "        img = np.zeros([height, width])\n",
    "        for j in range(num_filters):\n",
    "            r = int(j / cols) * (k + border)\n",
    "            c = int(j % cols) * (k + border)\n",
    "            img[r:r+k,c:c+k] = w[j,i]\n",
    "        filename = '%s_epoch_%02d_step_%06d_input_%03d.png' % (name, epoch, step, i)\n",
    "        ski.io.imsave(os.path.join(save_dir, filename), img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "train(session, train_x, train_y, valid_x, valid_y, config, net)\n",
    "evaluate(session, \"Test\", test_x, test_y, config, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Tensorflow CIFAR 10\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset contains 50000 images for training and validation, and 10000 test images. The images have dimensions 32x32 and they belong to 10 classes. Download the dataset version for Python [here](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz). Use the following code to load and prepair the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def shuffle_data(data_x, data_y):\n",
    "    indices = np.arange(data_x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_data_x = np.ascontiguousarray(data_x[indices])\n",
    "    shuffled_data_y = np.ascontiguousarray(data_y[indices])\n",
    "    return shuffled_data_x, shuffled_data_y\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "''''''\n",
    "DATA_DIR = 'CIFAR10 original'\n",
    "SAVE_DIR = 'task4_out'\n",
    "import os\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "num_channels = 3\n",
    "\n",
    "config = {}\n",
    "config['max_epochs'] = 8\n",
    "config['batch_size'] = 50\n",
    "config['save_dir'] = SAVE_DIR\n",
    "config['weight_decay'] = 1e-3\n",
    "config['lr_policy'] = {1:{'lr':1e-1}, 3:{'lr':1e-2}, 5:{'lr':1e-3}, 7:{'lr':1e-4}}\n",
    "\n",
    "\n",
    "''''''\n",
    "\n",
    "train_x = np.ndarray((0, img_height * img_width * num_channels), dtype=np.float32)\n",
    "train_y = []\n",
    "for i in range(1, 6):\n",
    "    subset = unpickle(os.path.join(DATA_DIR, 'data_batch_%d' % i))\n",
    "    train_x = np.vstack((train_x, subset['data']))\n",
    "    train_y += subset['labels']\n",
    "train_x = train_x.reshape((-1, num_channels, img_height, img_width)).transpose(0,2,3,1)\n",
    "train_y = np.array(train_y, dtype=np.int32)\n",
    "\n",
    "subset = unpickle(os.path.join(DATA_DIR, 'test_batch'))\n",
    "test_x = subset['data'].reshape((-1, num_channels, img_height, img_width)).transpose(0,2,3,1).astype(np.float32)\n",
    "test_y = np.array(subset['labels'], dtype=np.int32)\n",
    "\n",
    "valid_size = 5000\n",
    "train_x, train_y = shuffle_data(train_x, train_y)\n",
    "valid_x = train_x[:valid_size, ...]\n",
    "valid_y = train_y[:valid_size, ...]\n",
    "train_x = train_x[valid_size:, ...]\n",
    "train_y = train_y[valid_size:, ...]\n",
    "data_mean = train_x.mean((0,1,2))\n",
    "data_std = train_x.std((0,1,2))\n",
    "\n",
    "train_x = (train_x - data_mean) / data_std\n",
    "valid_x = (valid_x - data_mean) / data_std\n",
    "test_x = (test_x - data_mean) / data_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SparseSoftmaxCrossEntropyWithLogits(logits, labels):\n",
    "    '''\n",
    "    NOTE: For this operation, the probability of a given label is considered exclusive.\n",
    "    That is, soft classes are not allowed, and the labels vector must provide a single specific\n",
    "    index for the true class for each row of logits (each minibatch entry). For soft softmax\n",
    "    classification with a probability distribution for each entry, see softmax_cross_entropy_with_logits.\n",
    "    '''\n",
    "    # CIFAR-10 not in one-hot format\n",
    "    return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to train a convolutional model in Tensorflow. We propose a simple model which should yield about 70% accuracy in image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv(16,5) -> relu() -> pool(3,2) -> conv(32,5) -> relu() -> pool(3,2) -> fc(256) -> relu() -> fc(128) -> relu() -> fc(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `conv(16,5)` represents a convolution with 16 feature maps and filter dimensions 5x5, `pool(3,2)` is a max-pooling layer operating on patches 3x3 and the stride 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Dimensions of the tensor are: [batch, height, width, channels]\n",
    "##\n",
    "\n",
    "n_classes = 10 # CIFAR-10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# CIFAR-10: 32x32px images\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "Yoh_ = tf.placeholder(tf.int32, [None,])\n",
    "\n",
    "net=X\n",
    "regularizers=0\n",
    "\n",
    "#conv(16,5) represents a convolution with 16 feature maps and filter dimensions 5x5\n",
    "#conv(16,5)\n",
    "w_conv1 = tf.get_variable('w_conv1', [5, 5, 3, 16], initializer=xavier_initializer_conv2d())\n",
    "b_conv1 = tf.Variable(tf.zeros([16]), name='b_conv1')\n",
    "net = Convolution(net, w_conv1, b_conv1)\n",
    "regularizers += L2Regularizer(w_conv1)\n",
    "\n",
    "#relu()\n",
    "net = ReLU(net)\n",
    "\n",
    "#pool(3,2) is a max-pooling layer operating on patches 3x3 and the stride 2\n",
    "#pool(3,2)\n",
    "net = MaxPooling(net, 3, 2)\n",
    "\n",
    "#conv(32,5)\n",
    "w_conv2 = tf.get_variable('w_conv2', [5, 5, 16, 32], initializer=xavier_initializer_conv2d())\n",
    "b_conv2 = tf.Variable(tf.zeros([32]), name='b_conv2')\n",
    "net = Convolution(net, w_conv2, b_conv2)\n",
    "regularizers += L2Regularizer(w_conv2)\n",
    "\n",
    "#relu()\n",
    "net = ReLU(net)\n",
    "\n",
    "#pool(3,2)\n",
    "net = MaxPooling(net, 3, 2)\n",
    "\n",
    "#fc(256)\n",
    "w_fc1 = tf.get_variable('w_fc1', [8*8*32, 256], initializer=xavier_initializer())\n",
    "b_fc1 = tf.Variable(tf.zeros([256]), name='b_fc1')\n",
    "net = Flatten(net, w_fc1)\n",
    "net = FC(net, w_fc1, b_fc1)\n",
    "regularizers += L2Regularizer(w_fc1)\n",
    "\n",
    "#relu()\n",
    "net = ReLU(net)\n",
    "\n",
    "#fc(128)\n",
    "w_fc2 = tf.get_variable('w_fc2', [256, 128], initializer=xavier_initializer())\n",
    "b_fc2 = tf.Variable(tf.zeros([128]), name='b_fc2')\n",
    "net = FC(net, w_fc2, b_fc2)\n",
    "regularizers += L2Regularizer(w_fc2)\n",
    "\n",
    "#relu()\n",
    "net = ReLU(net)\n",
    "\n",
    "#fc(10)\n",
    "w_fc3 = tf.get_variable('w_fc3', [128, n_classes], initializer=xavier_initializer())\n",
    "b_fc3 = tf.Variable(tf.zeros([n_classes]), name='b_fc3')\n",
    "net = FC(net, w_fc3, b_fc3)\n",
    "\n",
    "\n",
    "loss_per_sample = SparseSoftmaxCrossEntropyWithLogits(net, Yoh_)\n",
    "loss_mean = ReduceMean(loss_per_sample)\n",
    "loss = RegularizedLoss(loss_mean, regularizers)\n",
    "\n",
    "#lr = tf.placeholder(tf.float32) # learning rate\n",
    "#train_step =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.01, global_step, 900, 0.9, staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the function `evaluate` which compares the predicted and correct class indices and determines the following classification performance indicators: overall classification accuracy, confusion matrix, as well as precision and recall for particular classes. In the implementation, first determine the confusion matrix, and then use it to determine all other indicators. During training, invoke `evaluate` after each epoch both on the training and on the validation dataset, and graph the average loss, the training rate and overall classification accuracy. We recommend that function receives the data, the correct class indices, and the required tensorflow operations. Be careful not to invoke the training operation. The function should output the recovered indicators to the console.\n",
    "\n",
    "> A typical loss graph when the training proceeds well.\n",
    "\n",
    "Visualize random initializations and the trained filters from the first layer. You can access the variable which holds the weight of the first layer by invoking the `tf.contrib.framework.get_variables` method with the scope in which the variable is used in the model. We supply an example of how that might look like below. The scope will depend on the code which you actually used while defining the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### don't run!\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "conv1_var = tf.contrib.framework.get_variables('model/conv1_1/weights:0')[0]\n",
    "conv1_weights = conv1_var.eval(session=sess)\n",
    "draw_conv_filters(0, 0, conv1_weights, SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide code which you can use for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_conv_filters(epoch, step, weights, save_dir):\n",
    "    w = weights.copy()\n",
    "    num_filters = w.shape[3]\n",
    "    num_channels = w.shape[2]\n",
    "    k = w.shape[0]\n",
    "    assert w.shape[0] == w.shape[1]\n",
    "    w = w.reshape(k, k, num_channels, num_filters)\n",
    "    w -= w.min()\n",
    "    w /= w.max()\n",
    "    border = 1\n",
    "    cols = 8\n",
    "    rows = math.ceil(num_filters / cols)\n",
    "    width = cols * k + (cols-1) * border\n",
    "    height = rows * k + (rows-1) * border\n",
    "    img = np.zeros([height, width, num_channels])\n",
    "    for i in range(num_filters):\n",
    "        r = int(i / cols) * (k + border)\n",
    "        c = int(i % cols) * (k + border)\n",
    "        img[r:r+k,c:c+k,:] = w[:,:,:,i]\n",
    "    filename = 'epoch_%02d_step_%06d.png' % (epoch, step)\n",
    "    ski.io.imsave(os.path.join(save_dir, filename), img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CIFAR-10: random initializations (top) and the learned filters in the first layer (bottom) with regularization lambda = 0.0001.\n",
    "    \n",
    "\n",
    "Visualize 20 incorrectly classified images with the largest loss and output the correct class and the top 3 predicted classes. Pay attention that in order to visualize image, you first need to undo the normalization of the mean value and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_image(img, mean, std):\n",
    "    img = img.copy() ## if we don't copy, further operations with stddev and mean are done on input data\n",
    "    img *= std\n",
    "    img += mean\n",
    "    img = img.astype(np.uint8)\n",
    "    ski.io.imshow(img)\n",
    "    ski.io.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the code for producing graphs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_training_progress(save_dir, data):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16,8))\n",
    "\n",
    "    linewidth = 2\n",
    "    legend_size = 10\n",
    "    train_color = 'm'\n",
    "    val_color = 'c'\n",
    "\n",
    "    num_points = len(data['train_loss'])\n",
    "    x_data = np.linspace(1, num_points, num_points)\n",
    "    ax1.set_title('Cross-entropy loss')\n",
    "    ax1.plot(x_data, data['train_loss'], marker='o', color=train_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='train')\n",
    "    ax1.plot(x_data, data['valid_loss'], marker='o', color=val_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='validation')\n",
    "    ax1.legend(loc='upper right', fontsize=legend_size)\n",
    "    ax2.set_title('Average class accuracy')\n",
    "    ax2.plot(x_data, data['train_acc'], marker='o', color=train_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='train')\n",
    "    ax2.plot(x_data, data['valid_acc'], marker='o', color=val_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='validation')\n",
    "    ax2.legend(loc='upper left', fontsize=legend_size)\n",
    "    ax3.set_title('Learning rate')\n",
    "    ax3.plot(x_data, data['lr'], marker='o', color=train_color,\n",
    "                     linewidth=linewidth, linestyle='-', label='learning_rate')\n",
    "    ax3.legend(loc='upper left', fontsize=legend_size)\n",
    "\n",
    "    save_path = os.path.join(save_dir, 'training_plot.pdf')\n",
    "    print('Plotting in: ', save_path)\n",
    "    plt.savefig(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(session, train_x, train_y, valid_x, valid_y, config, logits):\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    lr_policy = config['lr_policy']\n",
    "    batch_size = config['batch_size']\n",
    "    max_epochs = config['max_epochs']\n",
    "    save_dir = config['save_dir']\n",
    "    num_examples = train_x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    \n",
    "    \n",
    "    plot_data = {}\n",
    "    plot_data['train_loss'] = []\n",
    "    plot_data['valid_loss'] = []\n",
    "    plot_data['train_acc'] = []\n",
    "    plot_data['valid_acc'] = []\n",
    "    plot_data['lr'] = []\n",
    "    for epoch_num in range(1, max_epochs + 1):\n",
    "        train_x, train_y = shuffle_data(train_x, train_y)\n",
    "        for step in range(num_batches):\n",
    "            offset = step * batch_size \n",
    "            # s ovim kodom pazite da je broj primjera djeljiv s batch_size\n",
    "            batch_x = train_x[offset:(offset + batch_size), ...]\n",
    "            batch_y = train_y[offset:(offset + batch_size),]\n",
    "            feed_dict = {X: batch_x, Yoh_: batch_y}\n",
    "            start_time = time.time()\n",
    "            run_ops = [train_step, loss, logits]\n",
    "            ret_val = session.run(run_ops, feed_dict=feed_dict)\n",
    "            _, loss_val, logits_val = ret_val\n",
    "            duration = time.time() - start_time\n",
    "            if (step+1) % 20 == 0:\n",
    "                sec_per_batch = float(duration)\n",
    "                format_str = 'epoch %d / %d, step %d / %d, loss = %.2f (%.3f sec/batch)'\n",
    "                print(format_str % (epoch_num, max_epochs+1, step+1, num_batches, loss_val, sec_per_batch))\n",
    "            if (step+1) % 100 == 0:\n",
    "                w = session.run(w_conv1)\n",
    "                draw_conv_filters(epoch_num, step+1, w, save_dir)\n",
    "\n",
    "        #print('Train error:')\n",
    "        train_loss, train_acc = evaluate(session, 'Train error', train_x, train_y, config, logits)\n",
    "        #print('Validation error:')\n",
    "        valid_loss, valid_acc = evaluate(session, 'Validation error', valid_x, valid_y, config, logits)\n",
    "        plot_data['train_loss'] += [train_loss]\n",
    "        plot_data['valid_loss'] += [valid_loss]\n",
    "        plot_data['train_acc'] += [train_acc]\n",
    "        plot_data['valid_acc'] += [valid_acc]\n",
    "        plot_data['lr'] += [learning_rate.eval(session=session)]\n",
    "        #plot_data['lr'] += [session.run(learning_rate)]\n",
    "        plot_training_progress(SAVE_DIR, plot_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(session, name, x, y, config, logits):\n",
    "    print(\"\\nRunning evaluation: \", name)\n",
    "    batch_size = config['batch_size']\n",
    "    num_examples = x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    cnt_correct = 0\n",
    "    loss_avg = 0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_x = x[i*batch_size:(i+1)*batch_size, ...]\n",
    "        batch_y = y[i*batch_size:(i+1)*batch_size,]\n",
    "        \n",
    "        ##logits = forward_pass(net, batch_x)\n",
    "        data_dict = {X: batch_x, Yoh_: batch_y}\n",
    "        logits_val, loss_val = session.run([logits, loss], feed_dict=data_dict)\n",
    "        \n",
    "        yp = np.argmax(logits_val, 1)\n",
    "        ##yt = np.argmax(batch_y, 1)\n",
    "        ## because array has only values\n",
    "        yt = batch_y\n",
    "        cnt_correct += (yp == yt).sum()\n",
    "        #####loss_val = session.run(loss, feed_dict=data_dict)\n",
    "        loss_avg += loss_val\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(\"step %d / %d, loss = %.2f\" % (i*batch_size, num_examples, loss_val / batch_size))\n",
    "    valid_acc = cnt_correct / num_examples * 100\n",
    "    loss_avg /= num_batches\n",
    "    print(name + \" accuracy = %.2f\" % valid_acc)\n",
    "    print(name + \" avg loss = %.2f\\n\" % loss_avg)\n",
    "    return loss_avg, valid_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc; gc.collect()\n",
    "session = tf.Session()\n",
    "train(session, train_x, train_y, valid_x, valid_y, config, net)\n",
    "evaluate(session, \"Test\", test_x, test_y, config, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def worst_samples(session, x, y, config, logits):\n",
    "    batch_size = config['batch_size']\n",
    "    num_examples = x.shape[0]\n",
    "    num_batches = num_examples // batch_size\n",
    "\n",
    "    worst_samples = []\n",
    "    for i in range(num_batches):\n",
    "        batch_x = x[i*batch_size:(i+1)*batch_size, ...]\n",
    "        batch_y = y[i*batch_size:(i+1)*batch_size,]\n",
    "\n",
    "        data_dict = {X: batch_x, Yoh_: batch_y}\n",
    "        loss_vals, logits_val = session.run([loss_per_sample, logits] ,feed_dict=data_dict)\n",
    "        prediction = np.argmax(logits_val, 1)\n",
    "        loss_pairs = [(i*batch_size+id, loss, p) for id, (loss, p) in enumerate(zip(loss_vals, prediction))]\n",
    "        worst_samples = sorted(loss_pairs + worst_samples, key=lambda x: -x[1])[:20]\n",
    "    return worst_samples\n",
    "\n",
    "worst = worst_samples(session, test_x, test_y, config, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = np.array(unpickle(os.path.join(DATA_DIR, 'batches.meta'))['label_names'])\n",
    "\n",
    "for sample_id, _, predicted in worst:\n",
    "    draw_image(test_x[sample_id], data_mean, data_std)\n",
    "    probs = session.run(tf.nn.softmax(net), feed_dict={X: np.array([test_x[sample_id]])})[0]\n",
    "    predictions  = np.argsort(-probs)\n",
    "\n",
    "    print('Correct class:')\n",
    "    print(class_names[test_y[sample_id]])\n",
    "    print()\n",
    "    print('Predicted classes:')\n",
    "    for i in range(3):\n",
    "        print('{} {:.1f}%'.format(class_names[predictions[i]], probs[predictions[i]]*100))\n",
    "    print();print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have access to a GPU, you might want to try obtaining better results with a more powerful model. In that case, [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130) you can find a review of state-of-the-art results on this dataset. As you see, best approaches achieve around 96% overall classification accuracy. Two important tricks to achieve this are image upsampling and jittering. Image upsampling ensures that early convolutions detect very low level features, while jittering prevents overfitting. Without these techniques, it will be very hard for you to achieve more than 90% overall classification accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
