{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Implementing the recurrent neural network\n",
    "The vanilla neural network we will implement has three hyperparameters - the size of the hidden layer, the number of timesteps and the learning rate. The optimization algorithm which you will implement is Adagrad.\n",
    "\n",
    "With the assumption of single precision of decimal numbers (4 bytes), and the size of the hidden layer of 500, the language modelling task with a vocabulary size of 70 (input and output dimension) - how large is the memory requirement of the parameters of the model? Which parameter takes up the most memory? Repeat the same exercise with a vocabulary of size 10000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_t = 4 # 4 bytes\n",
    "hidden_layer_size = 500\n",
    "vocabulary_size = 70 #10000\n",
    "input_dimension = vocabulary_size\n",
    "output_dimension = vocabulary_size\n",
    "\n",
    "'''\n",
    "Parameters are: U, V, W\n",
    "'''\n",
    "print('Paramteres are: U, V, W, biases: b, c')\n",
    "print()\n",
    "\n",
    "w_size = hidden_layer_size*hidden_layer_size\n",
    "print('W.size = {}*{} = {}'.format(hidden_layer_size, hidden_layer_size, w_size))\n",
    "u_size = input_dimension*hidden_layer_size\n",
    "print('U.size = {}*{} = {}'.format(input_dimension, hidden_layer_size, u_size))\n",
    "v_size = hidden_layer_size*output_dimension\n",
    "print('V.size = {}*{} = {}'.format(hidden_layer_size, output_dimension, v_size))\n",
    "b_size = vocabulary_size\n",
    "print('b.size = {}'.format(vocabulary_size))\n",
    "c_size = output_dimension\n",
    "print('c.size = {}'.format(c_size))\n",
    "total_size = w_size+u_size+v_size+b_size+c_size\n",
    "print('total size = {}+{}+{}+{}+{} = {}'.format(w_size,u_size,v_size,b_size,c_size, total_size))\n",
    "print('total size in bytes = {}*{} = {}'.format(total_size, float_t, total_size*float_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aforementioned vocabulary sizes are the one you will use in the task of character level language modelling (approx. 70), and for word level language models (commonly much larger than $10^5$).\n",
    "\n",
    "The initial hyperparameters are the hidden layer size of 100, an unroll of 30 timesteps and a learning rate of 1e-1. You should try out some other hyperparameters as well and report on loss and sample convergence.\n",
    "\n",
    "The implementation of the recurrent neural network can be split conceptually into four parts, as follows:\n",
    "\n",
    "### 2.1.1: Parameter initialization\n",
    "\n",
    "The parameter matrices should be initialized randomly from a gaussian distribution with a standard deviation of 1e-2. The bias vectors should be initialized to zeros.\n",
    "\n",
    "Parameter initializtion code could look as follows:\n",
    "\n",
    "```\n",
    "def __init__(self, hidden_size, sequence_length, vocab_size, learning_rate):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Remember to add a redundant dimension while initializing bias vectors for numpy broadcasting.\n",
    "\n",
    "### 2.1.2: Forward pass\n",
    "\n",
    "The forward pass of the recurrent neural network can be imagined as a loop in which we iteratively perform a single timestep. The forward pass can therefore be implemented as a function which processes a single timestep, and a function which iterates over the whole sequence and stores the results.\n",
    "\n",
    "The skeletons of the aforementioned functions could look as follows:\n",
    "\n",
    "```\n",
    "def rnn_step_forward(self, x, h_prev, U, W, b):\n",
    "    ...\n",
    "\n",
    "def rnn_forward(self, x, h0, U, W, b):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Notice that even though the parameters U, W and b exist as instance variables of the class (self.U, self.W, self.b), due to reproducibility we redundantly leave them as arguments of the functions.\n",
    "\n",
    "### 2.1.3: Backward pass\n",
    "\n",
    "The backward pass of the recurrent network is conceptually similar to the forward pass with the iteration being done in reverse, from the last timestep towards the first one. The backward pass is done by the backpropagation through time algorithm (BPTT). The skeletons of the BPTT algorithm functions could look as follows:\n",
    "\n",
    "```\n",
    "def rnn_step_backward(self, grad_next, cache):\n",
    "    ...\n",
    "    \n",
    "def rnn_backward(self, dh, cache):\n",
    "    ...\n",
    "```\n",
    "\n",
    "In order to mitigate the exploding gradient problem, before applying the accumulated gradient, apply elementwise gradient clipping. We recommend you use the `np.clip()` method, and leave the values in the interval of $[−5,5]$.\n",
    "\n",
    "### 2.1.4: Training loop\n",
    "\n",
    "The training loop connects the recurrent network with the input data, calculates the loss and does parameter optimization. Ideally, the network should be idependent of the data - so it is recommended to separate the iteration over batches and epochs from the network class.\n",
    "\n",
    "A skeleton implementation of the loss computation function could look as follows:\n",
    "\n",
    "```\n",
    "def output(h, V, c):\n",
    "    ...\n",
    "\n",
    "def output_loss_and_grads(self, h, V, c, y):\n",
    "    ...\n",
    "```\n",
    "\n",
    "The implementation of the parameter learning function could look as follows:\n",
    "\n",
    "```\n",
    "def update(self, dU, dW, db, dV, dc,\n",
    "    ...\n",
    "```\n",
    "\n",
    "The implementation of the control flow loop and iterating the optimization process could look as follows:\n",
    "\n",
    "```\n",
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=100)\n",
    "    ...\n",
    "```\n",
    "\n",
    "The `step` function, whose skeleton is not described, contains the logic of running a single forward and backward pass of a neural network on a minibatch of inputs. As inputs, the method accepts an initial hidden state vector, the one-hot encoded inputs and outputs of shapes BxTxV, where B is the minibatch size, T the number of timesteps to unroll the network for and V the size of the vocabulary. As outputs we receive the loss in that step as well as the hidden state from the last timestep, which should then be used as the next initial hidden state.\n",
    "\n",
    "Important: Think about how the data has to be aligned in order for the last hidden states from the previous step to be valid initial hidden states for the current step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @lab1:\n",
    "def softmax(_input):\n",
    "    expinput = np.exp(_input)\n",
    "    sumexp = expinput.sum(axis=1)\n",
    "    return (expinput.transpose() / sumexp).transpose()\n",
    "\n",
    "## @lab0\n",
    "# stable softmax\n",
    "def stable_softmax(x):\n",
    "    exp_x_shifted = np.exp(x - np.max(x))\n",
    "    probs = exp_x_shifted / np.sum(exp_x_shifted)\n",
    "    return probs\n",
    "\n",
    "softmax = stable_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class RNN:\n",
    "    def __init__(self, hidden_size, sequence_length, vocab_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # numpy.random.normal(loc=0.0, scale=1.0, size=None)\n",
    "        # Draw random samples from a normal (Gaussian) distribution.\n",
    "        _input_size = self.vocab_size # variables just to better understand\n",
    "        _output_size = self.vocab_size\n",
    "        self.U = np.random.normal(size=[_input_size, hidden_size], scale=1.0 / np.sqrt(hidden_size)) # ... input projection\n",
    "        self.W = np.random.normal(size=[hidden_size, hidden_size], scale=1.0 / np.sqrt(hidden_size)) # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros((1, self.hidden_size)) # ... input bias\n",
    "\n",
    "        self.V = np.random.normal(size=[hidden_size, _output_size], scale=1.0 / np.sqrt(_output_size)) # ... output projection\n",
    "        self.c = np.zeros((1, self.vocab_size)) # ... output bias\n",
    "\n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U, self.memory_W, self.memory_V = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        self.memory_b, self.memory_c = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "\n",
    "\n",
    "    def rnn_step_forward(self, x, h_prev, U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "\n",
    "        h_current = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b)\n",
    "        cache = (x, h_prev, h_current)\n",
    "\n",
    "        # return the new hidden state and a tuple of values needed for the backward step\n",
    "\n",
    "        return h_current, cache\n",
    "\n",
    "\n",
    "    def rnn_forward(self, x, h0, U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "\n",
    "        h = [h0]\n",
    "        cache = []\n",
    "        for seq in range(self.sequence_length):\n",
    "            input_data = x[:, seq, :]\n",
    "            h_current, cache_current = self.rnn_step_forward(input_data, h[-1], U, W, b)\n",
    "            h.append(h_current)\n",
    "            cache.append(cache_current)\n",
    "\n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "        #h = h[1:] # omit initial state h0\n",
    "        h = np.array(h[1:]).transpose((1, 0, 2))\n",
    "        return h, cache\n",
    "\n",
    "\n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        x, h_prev, h_current = cache\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        ## derivative of tanh(x) = 1-tanh^2(x)\n",
    "        ## http://math2.org/math/derivatives/more/hyperbolics.htm\n",
    "        grad_next = grad_next * (1 - h_current*h_current)\n",
    "\n",
    "        # cache - cached information from the forward pass\n",
    "\n",
    "        ##dh_prev, dU, dW, db = None, None, None, None\n",
    "        dh_prev = np.dot(grad_next, self.W.transpose())\n",
    "        dU = np.dot(x.transpose(), grad_next)\n",
    "        dW = np.dot(h_prev.transpose(), grad_next)\n",
    "        db = np.sum(grad_next, axis=0)\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # HINT: you can use the chain rule to compute the derivative of the\n",
    "        # hyperbolic tangent function and use it to compute the gradient\n",
    "        # with respect to the remaining parameters\n",
    "        \n",
    "        '''\n",
    "        In order to mitigate the exploding gradient problem, before applying the accumulated\n",
    "        gradient, apply elementwise gradient clipping. We recommend you use the np.clip()\n",
    "        method, and leave the values in the interval of [−5,5].\n",
    "        '''\n",
    "        dh_prev = np.clip(dh_prev, -5, 5)\n",
    "        dU = np.clip(dU, -5, 5)\n",
    "        dW = np.clip(dW, -5, 5)\n",
    "        db = np.clip(db, -5, 5)\n",
    "\n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        ###dU, dW, db = None, None, None\n",
    "\n",
    "        dh_prev = 0\n",
    "        dU_sum = 0\n",
    "        dW_sum = 0\n",
    "        db_sum = 0\n",
    "\n",
    "        for dh_elem, cache_elem in reversed(list(zip(dh, cache))):\n",
    "            dh_prev, dU, dW, db = self.rnn_step_backward(dh_elem + dh_prev, cache_elem)\n",
    "            dU_sum += dU\n",
    "            dW_sum += dW\n",
    "            db_sum += db\n",
    "        return dU_sum, dW_sum, db_sum\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        # Why are we not computing the gradient with respect to inputs (x)?\n",
    "        #return dU, dW, db\n",
    "\n",
    "\n",
    "    def output(self, h, V, c):\n",
    "        # Calculate the output probabilities of the network\n",
    "        return softmax(np.dot(h, V) + c)\n",
    "\n",
    "\n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a tensor of dimension \n",
    "        #     batch_size x sequence_length x vocabulary size - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[batch_id][timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[batch_id][timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a list or a dictionary.\n",
    "\n",
    "        #loss, dh, dV, dc = None, None, None, None\n",
    "        loss = 0.0\n",
    "        dh = []\n",
    "        dV = np.zeros_like(self.V)\n",
    "        dc = np.zeros_like(self.c)\n",
    "\n",
    "        # calculate the output (o) - unnormalized log probabilities of classes\n",
    "        # calculate yhat - softmax of the output\n",
    "        # calculate the cross-entropy loss\n",
    "        # calculate the derivative of the cross-entropy softmax loss with respect to the output (o)\n",
    "        # calculate the gradients with respect to the output parameters V and c\n",
    "        # calculate the gradients with respect to the hidden layer h\n",
    "        batch_size = len(h)\n",
    "\n",
    "        for t in range(self.sequence_length):\n",
    "            yp = y[:, t, :]\n",
    "            h_t = h[:, t, :]\n",
    "\n",
    "            o = self.output(h_t, V, c)\n",
    "            s = softmax(o)\n",
    "\n",
    "            loss += -np.sum(np.log(s)*yp) / batch_size\n",
    "            dO = (s - yp) / batch_size\n",
    "\n",
    "            dV += np.dot(h_t.T, dO)\n",
    "            dc += np.sum(dO, axis=0)\n",
    "\n",
    "            dh_t = np.dot(dO, V.T)\n",
    "            dh.append(dh_t)\n",
    "\n",
    "        return loss, dh, dV, dc\n",
    "\n",
    "\n",
    "    # The inputs to the function are just indicative since the variables are mostly present as class properties\n",
    "    def update(self, dU, dW, db, dV, dc,\n",
    "                     U, W, b, V, c,\n",
    "                     memory_U, memory_W, memory_b, memory_V, memory_c):\n",
    "\n",
    "        # update memory matrices\n",
    "        memory_U += np.square(dU)\n",
    "        memory_W += np.square(dW)\n",
    "        memory_b += np.square(db)\n",
    "        memory_V += np.square(dV)\n",
    "        memory_c += np.square(dc)\n",
    "\n",
    "        # perform the Adagrad update of parameters\n",
    "        epsilon = 1e-6\n",
    "        U -= self.learning_rate * dU / np.sqrt(memory_U + epsilon)\n",
    "        W -= self.learning_rate * dW / np.sqrt(memory_W + epsilon)\n",
    "        b -= self.learning_rate * db / np.sqrt(memory_b + epsilon)\n",
    "        V -= self.learning_rate * dV / np.sqrt(memory_V + epsilon)\n",
    "        c -= self.learning_rate * dc / np.sqrt(memory_c + epsilon)\n",
    "\n",
    "\n",
    "    def step(self, h0, x_oh, y_oh):\n",
    "        '''\n",
    "        The step function, whose skeleton is not described, contains the logic\n",
    "        of running a single forward and backward pass of a neural network on a minibatch\n",
    "        of inputs. As inputs, the method accepts an initial hidden state vector, the\n",
    "        one-hot encoded inputs and outputs of shapes BxTxV, where B is the minibatch\n",
    "        size, T the number of timesteps to unroll the network for and V the size of the\n",
    "        vocabulary. As outputs we receive the loss in that step as well as the hidden\n",
    "        state from the last timestep, which should then be used as the next initial hidden state.\n",
    "        '''\n",
    "        h_next, cache = self.rnn_forward(x_oh, h0, self.U, self.W, self.b)\n",
    "        loss, dh, dV, dc = self.output_loss_and_grads(h_next, self.V, self.c, y_oh)\n",
    "        dU, dW, db = self.rnn_backward(dh, cache)\n",
    "        self.update(dU, dW, db, dV, dc,\n",
    "                    self.U, self.W, self.b, self.V, self.c,\n",
    "                    self.memory_U, self.memory_W, self.memory_b, self.memory_V, self.memory_c)\n",
    "        return loss, h_next[:, -1, :]\n",
    "\n",
    "\n",
    "    # ...\n",
    "    # code not necessarily nested in class definition\n",
    "    def sample(self, seed, n_sample):\n",
    "        ##h0, seed_onehot, sample = None, None, None \n",
    "        # inicijalizirati h0 na vektor nula\n",
    "        # seed string pretvoriti u one-hot reprezentaciju ulaza\n",
    "        h0 = np.zeros((dataset.batch_size, self.hidden_size))\n",
    "\n",
    "        # seed to one hot\n",
    "        seed_onehot = to_one_hot(seed, self.vocab_size)\n",
    "\n",
    "        sample = seed\n",
    "\n",
    "        return sample\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Sampling data from the learned network\n",
    "\n",
    "When training the network, we implicitly take the next character as the next input instead of the one predicted by the network. This causes our loss to be artificially smaller, and is not applicable during testing (when we do not know the correct character).\n",
    "\n",
    "In the scope of the lab assignment we will not dive deep into sequence sampling, but implement the bare minimum. Every `sample_every` minibatches, you should run the sampling method from our recurrent neural network in the following fashion:\n",
    "\n",
    "1. Store the current hidden state of the network (`h_train`)\n",
    "2. Initialize an empty hidden state for sampling (`h0_sample`)\n",
    "3. Define a seed sequence of symbols to \"warm up\" the network - ex: `seed ='HAN:\\nIs that good or bad?\\n\\n'`\n",
    "4. Define how many characters you will sample (`n_sample=300`)\n",
    "5. Run the forward pass on the seed sequence \"seed\"\n",
    "6. Sample the remaining `n_sample` - `len(seed)` characters by using roulette-wheel (probability-proportional) sampling\n",
    "\n",
    "The skeleton of the sampling function could look as follows:\n",
    "\n",
    "```\n",
    "def sample(seed, n_sample):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# @lab1:\n",
    "'''\n",
    "    # instantiate the data X and the labels Yoh_\n",
    "    X = np.array([[1,3],[1,1],[3,2],[3,3]])\n",
    "    Y_ = np.array([1,0,2,2])\n",
    "\n",
    "    # Yoh c1 c2 c3\n",
    "    #  x1 0  1  0\n",
    "    #  x2 1  0  0\n",
    "    #  x3 0  0  1\n",
    "    #  x4 0  0  1\n",
    "\n",
    "    Yoh_ = np.zeros((len(X), len(np.bincount(Y_))))\n",
    "    Yoh_[range(len(Y_)), Y_] = 1\n",
    "'''\n",
    "def to_one_hot(x, vocab_size):\n",
    "    x_onehot = np.zeros((len(x), vocab_size))\n",
    "    x_onehot[range(len(x)), x] = 1\n",
    "    return x_onehot\n",
    "\n",
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=100):\n",
    "\n",
    "    vocab_size = len(dataset.sorted_chars)\n",
    "    rnn = RNN(hidden_size, sequence_length, vocab_size, learning_rate) # initialize the recurrent network\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "\n",
    "    h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "\n",
    "    average_loss = 0\n",
    "\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "\n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "            # why do we reset the hidden state here?\n",
    "            ## new sequence start\n",
    "\n",
    "        # One-hot transform the x and y batches\n",
    "        ## list comprehension because of batch size - several arrays in one unit\n",
    "        x_oh = np.array([to_one_hot(_x, vocab_size) for _x in x])\n",
    "        y_oh = np.array([to_one_hot(_y, vocab_size) for _y in y])\n",
    "\n",
    "        # Run the recurrent network on the current batch\n",
    "        # Since we are using windows of a short length of characters,\n",
    "        # the step function should return the hidden state at the end\n",
    "        # of the unroll. You should then use that hidden state as the\n",
    "        # input for the next minibatch. In this way, we artificially\n",
    "        # preserve context between batches.\n",
    "        loss, h0 = rnn.step(h0, x_oh, y_oh)\n",
    "\n",
    "        average_loss += loss\n",
    "\n",
    "        if batch % sample_every == 0: \n",
    "            # run sampling (2.2)\n",
    "            fmtstr = 'epoch={}/{}, batch={}/{}, batch loss={:.3f}, cumulative loss={:.3f}'\n",
    "            #print(\"Epoch = \",current_epoch,\"/\",max_epochs,\", Batch \",batch,\"/\",dataset.num_batches,\", Loss = \",average_loss)\n",
    "            print(fmtstr.format(current_epoch, max_epochs, batch, dataset.num_batches, loss, average_loss/batch))\n",
    "            #sample = rnn.sample(dataset.encode(\"HAN:\\nIs that good or bad?\\n\\n\"), 200)\n",
    "            #print(''.join(dataset.decode(sample)))\n",
    "            \n",
    "        batch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(10)\n",
    "    dataset = Dataset(minibatch_size=5, sequence_length=30)\n",
    "    dataset.preprocess('data/selected_conversations.txt')\n",
    "    dataset.create_minibatches()\n",
    "    run_language_model(dataset, 1000, sequence_length=dataset.sequence_length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
