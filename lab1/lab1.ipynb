{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generating data which is not linearly separable\n",
    "\n",
    "In our experiments in lab 0, logistic regression achieved quite good classification results. This should not be viewed as a surprise since the sigmoid of affine transformed data was a good fit for the chosen posterior probability of data classes. Our experiments slightly digressed from ideal theoretical assumptions (our classes had different covariances) but results showed that this digression did not hurt the performance of our algorithm. Now we will make the things a little harder by instantiating the data with a more complex generative model.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Write a subroutine `sample_gmm_2d(K, C, N)` which creates `K` random bivariate Gaussian distributions, and samples `N` data points from each of them. Unlike in `sample_gauss_2d`, here we need to assign class `c_i` to each bivariate distribution `G_i`, where c_i is randomly sampled from the set `{0, 1, ..., C-1}`. This way we get the data generated by mixtures of randomly chosen Gaussian distributions. The function should return a data matrix `X` and the groundtruth class matrix `Y`. The rows of both matrices correspond to sampled data points. The matrix `X` contains the data, while Y contains the class index of the generating distribution.\n",
    "- The implementation of the subroutine first should create random distributions and assign them a random class from 0 to `C - 1`. Then, it must sample the required number of data points from each of the distributions and assign these data the corresponding class index.\n",
    "- The subroutine should return the following data:\n",
    "\n",
    "```python\n",
    "X  ... data in a matrix [K·N x 2 ]\n",
    "Y_ ... class indices of data [K·N]\n",
    "```\n",
    "\n",
    "Run the subroutine `sample_gmm_2d` and test it by invoking drawing functions developed in lab exercise 0 (`graph_surface` and `graph_data`). Depending on the parameters and the state of the random number generator, your result could look similar to the figure below. Our parameters were: `K=4`, `C=2`, `N=30`.\n",
    "\n",
    "-- image\n",
    "\n",
    "When you are satisfied with the execution results, save the code in the file `data.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multi layer classification in Python\n",
    "\n",
    "In this exercise you shall develop an algorithm for learning a probabilistic classification model with one hidden layer, by employing the negative log-likelihood loss and stochastic gradient descent. Your algorithm should be stored in the module `fcann2`. The organization of the module should follow the example set by the module logreg from the lab exercise 0. The module should contain the methods `fcann2_train` and `fcann2_classify`. The module should be tested on an artificial dataset containing 2D data of two classes sampled from a Gaussian mixtures of 6 components.\n",
    "\n",
    "Depending on parameters and the seed of the random number generator, your result could look like in the figure below. Our hyper-parameters were: `K=6`, `C=2`, `N=10`, `param_niter=1e5`, `param_delta=0.05`, `param_lambda=1e-3` (regularization coefficient), hidden layer dimensionality: `5`.\n",
    "\n",
    "-- image\n",
    "\n",
    "When you are satisfied with the execution results, save the code in the file `fcann2.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear regression in Tensorflow\n",
    "\n",
    "We illustrate a typical structure of a machine learning algorithm in Tensorflow on a complete example of the optimization procedure for estimating the parameters of a line `y = a * x + b` passing through points `(1,3)` and `(2,5)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 9. 25.] 2.6000001 1.6\n",
      "1 [1.4400007 3.2400007] 1.64 0.99999994\n",
      "2 [0.1296001 0.5184004] 2.0 1.2160001\n",
      "3 [0.04665603 0.04665603] 1.8704 1.1296\n",
      "4 [0.         0.01679617] 1.92224 1.1555201\n",
      "5 [0.00604665 0.        ] 1.906688 1.139968\n",
      "6 [0.00217679 0.00217679] 1.9160192 1.139968\n",
      "7 [0.00313458 0.00078365] 1.9160192 1.1343693\n",
      "8 [0.00253898 0.00112844] 1.9193784 1.13101\n",
      "9 [0.00253898 0.00091406] 1.9213941 1.1269791\n",
      "10 [0.00233997 0.000914  ] 1.9238124 1.123351\n",
      "11 [0.00222439 0.0008424 ] 1.9259894 1.1197231\n",
      "12 [0.00208963 0.0008008 ] 1.9281663 1.1162403\n",
      "13 [0.00197193 0.00075225] 1.9302559 1.1128445\n",
      "14 [0.00185764 0.00070989] 1.9322933 1.1095531\n",
      "15 [0.00175111 0.00066876] 1.9342681 1.1063559\n",
      "16 [0.00165032 0.0006304 ] 1.9361864 1.1032526\n",
      "17 [0.00155545 0.00059412] 1.9380484 1.1002398\n",
      "18 [0.00146598 0.00055996] 1.9398562 1.0973148\n",
      "19 [0.00138167 0.00052776] 1.9416112 1.0944753\n",
      "20 [0.00130224 0.00049741] 1.9433149 1.0917184\n",
      "21 [0.00122733 0.0004688 ] 1.9449689 1.0890422\n",
      "22 [0.00115676 0.00044184] 1.9465747 1.0864439\n",
      "23 [0.00109023 0.00041643] 1.9481337 1.0839216\n",
      "24 [0.00102755 0.00039248] 1.9496471 1.0814728\n",
      "25 [0.00096844 0.00036992] 1.9511164 1.0790955\n",
      "26 [0.00091276 0.00034863] 1.9525427 1.0767874\n",
      "27 [0.00086025 0.0003286 ] 1.9539276 1.0745468\n",
      "28 [0.00081079 0.00030969] 1.9552721 1.0723716\n",
      "29 [0.00076417 0.00029187] 1.956577 1.0702597\n",
      "30 [0.0007202  0.00027511] 1.9578441 1.0682096\n",
      "31 [0.00067881 0.00025927] 1.9590741 1.0662192\n",
      "32 [0.00063975 0.00024438] 1.9602685 1.0642871\n",
      "33 [0.00060298 0.00023031] 1.9614277 1.0624111\n",
      "34 [0.00056829 0.00021707] 1.9625533 1.06059\n",
      "35 [0.00053561 0.00020458] 1.9636459 1.058822\n",
      "36 [0.00050482 0.00019282] 1.9647067 1.0571057\n",
      "37 [0.00047578 0.00018174] 1.9657366 1.0554394\n",
      "38 [0.00044842 0.00017128] 1.9667363 1.0538217\n",
      "39 [0.00042263 0.00016144] 1.967707 1.0522512\n",
      "40 [0.00039833 0.00015215] 1.9686493 1.0507265\n",
      "41 [0.00037542 0.0001434 ] 1.9695641 1.0492463\n",
      "42 [0.00035383 0.00013515] 1.9704522 1.0478094\n",
      "43 [0.00033348 0.00012738] 1.9713144 1.0464144\n",
      "44 [0.00031431 0.00012005] 1.9721514 1.0450599\n",
      "45 [0.00029623 0.00011315] 1.972964 1.043745\n",
      "46 [0.00027919 0.00010664] 1.973753 1.0424685\n",
      "47 [0.00026314 0.00010051] 1.9745189 1.0412294\n",
      "48 [2.4800777e-04 9.4725903e-05] 1.9752623 1.0400263\n",
      "49 [2.3374103e-04 8.9283698e-05] 1.9759842 1.0388584\n",
      "50 [2.203001e-04 8.415114e-05] 1.976685 1.0377246\n",
      "51 [2.076349e-04 7.930634e-05] 1.9773653 1.0366238\n",
      "52 [1.9569133e-04 7.4745265e-05] 1.9780257 1.0355551\n",
      "53 [1.844381e-04 7.044728e-05] 1.9786668 1.0345176\n",
      "54 [1.7383229e-04 6.6400826e-05] 1.9792893 1.0335104\n",
      "55 [1.6383333e-04 6.2579624e-05] 1.9798937 1.0325327\n",
      "56 [1.5441483e-04 5.8981466e-05] 1.9804804 1.0315834\n",
      "57 [1.4553961e-04 5.5589338e-05] 1.98105 1.0306618\n",
      "58 [1.371671e-04 5.239426e-05] 1.981603 1.0297672\n",
      "59 [1.2928103e-04 4.9374125e-05] 1.9821397 1.0288985\n",
      "60 [1.2184414e-04 4.6541176e-05] 1.9826609 1.0280552\n",
      "61 [1.1483183e-04 4.3867680e-05] 1.983167 1.0272367\n",
      "62 [1.0823558e-04 4.1334555e-05] 1.983658 1.0264418\n",
      "63 [1.0200773e-04 3.8966002e-05] 1.9841349 1.0256703\n",
      "64 [9.6141994e-05 3.6719317e-05] 1.9845977 1.0249212\n",
      "65 [9.060873e-05 3.461224e-05] 1.9850472 1.024194\n",
      "66 [8.5398009e-05 3.2621865e-05] 1.9854836 1.023488\n",
      "67 [8.0491242e-05 3.0743282e-05] 1.9859072 1.0228026\n",
      "68 [7.5862488e-05 2.8976847e-05] 1.9863185 1.0221372\n",
      "69 [7.1499766e-05 2.7312490e-05] 1.9867178 1.0214913\n",
      "70 [6.7391433e-05 2.5736044e-05] 1.9871053 1.020864\n",
      "71 [6.3511012e-05 2.4262794e-05] 1.9874817 1.0202553\n",
      "72 [5.9863640e-05 2.2860315e-05] 1.9878467 1.0196642\n",
      "73 [5.6413628e-05 2.1552682e-05] 1.9882015 1.0190905\n",
      "74 [5.3173735e-05 2.0309335e-05] 1.9885458 1.0185335\n",
      "75 [5.0114002e-05 1.9140458e-05] 1.9888799 1.0179926\n",
      "76 [4.7233374e-05 1.8042661e-05] 1.9892044 1.0174676\n",
      "77 [4.4514323e-05 1.7004801e-05] 1.9895195 1.016958\n",
      "78 [4.1956140e-05 1.6024393e-05] 1.9898252 1.0164632\n",
      "79 [3.9545590e-05 1.5102728e-05] 1.9901221 1.0159827\n",
      "80 [3.7270369e-05 1.4237132e-05] 1.9904103 1.0155164\n",
      "81 [3.5124696e-05 1.3418031e-05] 1.9906902 1.0150636\n",
      "82 [3.3108448e-05 1.2643533e-05] 1.9909618 1.014624\n",
      "83 [3.1199714e-05 1.1918382e-05] 1.9912256 1.0141973\n",
      "84 [2.9409677e-05 1.1230619e-05] 1.9914814 1.013783\n",
      "85 [2.77126674e-05 1.05881045e-05] 1.9917301 1.0133809\n",
      "86 [2.6122107e-05 9.9795725e-06] 1.9919715 1.0129905\n",
      "87 [2.462112e-05 9.403666e-06] 1.9922057 1.0126114\n",
      "88 [2.3203575e-05 8.8647394e-06] 1.9924333 1.0122435\n",
      "89 [2.1872635e-05 8.3499799e-06] 1.9926538 1.0118861\n",
      "90 [2.0611296e-05 7.8746834e-06] 1.9928683 1.0115393\n",
      "91 [1.9427329e-05 7.4211057e-06] 1.9930764 1.0112027\n",
      "92 [1.8311011e-05 6.9935859e-06] 1.9932784 1.0108758\n",
      "93 [1.7257422e-05 6.5909844e-06] 1.9934745 1.0105585\n",
      "94 [1.6265805e-05 6.2121967e-06] 1.9936649 1.0102503\n",
      "95 [1.5329651e-05 5.8561509e-06] 1.9938498 1.0099512\n",
      "96 [1.4446605e-05 5.5195705e-06] 1.9940293 1.009661\n",
      "97 [1.3617891e-05 5.1994730e-06] 1.9942033 1.0093789\n",
      "98 [1.2832437e-05 4.9037080e-06] 1.9943726 1.0091053\n",
      "99 [1.2096833e-05 4.6207024e-06] 1.9945369 1.0088396\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## 1. definition of the computation graph\n",
    "# data and parameters\n",
    "X  = tf.placeholder(tf.float32, [None])\n",
    "Y_ = tf.placeholder(tf.float32, [None])\n",
    "a = tf.Variable(0.0)\n",
    "b = tf.Variable(0.0)\n",
    "\n",
    "# affine regression model\n",
    "Y = a * X + b\n",
    "\n",
    "# quadratic loss\n",
    "loss = (Y-Y_)**2\n",
    "\n",
    "# optimization by gradient descent\n",
    "trainer = tf.train.GradientDescentOptimizer(0.1)\n",
    "train_op = trainer.minimize(loss)\n",
    "\n",
    "## 2. parameter initialization\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "## 3. training\n",
    "# let the games start!\n",
    "for i in range(100):\n",
    "  val_loss, _, val_a,val_b = sess.run(\n",
    "      [loss, train_op, a,b], \n",
    "      feed_dict={X: [1,2], Y_: [3,5]})\n",
    "  print(i,val_loss, val_a,val_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignments:\n",
    "\n",
    "- Analyze the provided program and check the execution correctness.\n",
    "- Modify the program so that a line is fit to an arbitrary number of points. Ensure that the magnitude of the gradients does not depend on the count of the data.\n",
    "- [Express](https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#processing_gradients_before_applying_them) the optimization step by calling methods `compute_gradients` and `apply_gradients`.\n",
    "- Fetch and print the gradient values during optimization by evaluating (`run`) the gradients returned by `compute_gradients`. Take into account that `compute_gradients` returns a list of tuples (gradient, variable), while the method `run` receives a list of nodes in a single positional argument.\n",
    "- Define analytical expression for the loss gradient w.r.t. parameters `a` and `b`. Define the nodes which calculate the gradients explicitly, and evaluate them by invoking the method `run`. Print out values of the gradients and make sure they are equal to the values determined by Tensorflow.\n",
    "- Show that values of the gradients may be printed using the function [tf.Print](https://www.tensorflow.org/api_docs/python/tf/Print). \n",
    "\n",
    "When you are satisfied with the execution results, save the code in the file `fcann2.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Logistic regression in Tensorflow\n",
    "\n",
    "In this assignment we shall express the logistic regression under Tensorflow. The source code will be only half as long as the corresponding \"hand made\" code from the lab 0. The exercise will demonstrate the following Tensorflow advantages: i) we don't need to derive gradients, and ii) the code may be executed on different processing platforms (CPU, GPU) without making substantial changes. These advantages will be crucial in cases of large models with hundres of millions of parameters (in smaller models, a CPU implementation may be faster due to expensive transfers from RAM to GPU).\n",
    "\n",
    "In the introductory examples we have seen that Tensorflow programs often need to use two names for the same tensor. The first name references the node in the computation graph, whereas the second name references a value of that node for specific input data (eg. `loss` and `val_loss`, or a and val_a). To avoid the need for duplicated naming, the algorithm can be expressed as a class whose data members reference the graph nodes. This allows the client code to use the same names to refer to the specific tensor values. Following these guidelines, a class for logistic regression under Tensorflow could have the structure as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TFLogreg:\n",
    "  def __init__(self, D, C, param_delta=0.5):\n",
    "    \"\"\"Arguments:\n",
    "       - D: dimensions of each datapoint \n",
    "       - C: number of classes\n",
    "       - param_delta: training step\n",
    "    \"\"\"\n",
    "\n",
    "    # declare graph nodes for the data and parameters:\n",
    "    # self.X, self.Yoh_, self.W, self.b\n",
    "    # ...\n",
    "\n",
    "    # formulate the model: calculate self.probs\n",
    "    #   use tf.matmul, tf.nn.softmax\n",
    "    # ...\n",
    "\n",
    "    # formulate the loss: self.loss\n",
    "    #   use tf.log, tf.reduce_sum, tf.reduce_mean\n",
    "    # ...\n",
    "\n",
    "    # formulate the training operation: self.train_step\n",
    "    #   use tf.train.GradientDescentOptimizer,\n",
    "    #       tf.train.GradientDescentOptimizer.minimize\n",
    "    # ...\n",
    "\n",
    "    # instantiate the execution context: self.session\n",
    "    #   use tf.Session\n",
    "    # ...\n",
    "\n",
    "  def train(self, X, Yoh_, param_niter):\n",
    "    \"\"\"Arguments:\n",
    "       - X: actual datapoints [NxD]\n",
    "       - Yoh_: one-hot encoded labels [NxC]\n",
    "       - param_niter: number of iterations\n",
    "    \"\"\"\n",
    "    # parameter intiailization\n",
    "    #   use tf.global_variables_initializer\n",
    "    # ...\n",
    "\n",
    "    # optimization loop\n",
    "    #   use tf.Session.run\n",
    "    # ...\n",
    "\n",
    "  def eval(self, X):\n",
    "    \"\"\"Arguments:\n",
    "       - X: actual datapoints [NxD]\n",
    "       Returns: predicted class probabilites [NxC]\n",
    "    \"\"\"\n",
    "    #   use tf.Session.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that unlike the previous exercise the true labels of the training data are now called `Yoh_` instead of `Y_`. This is due to the fact that, in Tensorflow, the cross entropy loss is more easily expressed by organizing the labels in a matrix n which the rows correspond to data points while the columns correspond to class labels (this is also known as one-hot notation). If a data point x_i matches the class `c_j`, then `Yoh_[i,j]=1` and `Yoh_[i,k]=0` for all `k!=j` (\"one hot\"). In former mathematical discussion, data labels organized in such manner were referenced by the matrix $Y′$\n",
    "\n",
    "The structure of a test program should be very similar to the test programs from the previous exercise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-9485e260054b>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-9485e260054b>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    tflr.train(X, Yoh_, 1000)\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  # initialize the random number generator\n",
    "  np.random.seed(100)\n",
    "  tf.set_random_seed(100)\n",
    "\n",
    "  # instantiate the data X and the labels Yoh_\n",
    "\n",
    "  # build the graph:\n",
    "  tflr = TFLogreg(X.shape[1], Yoh_.shape[1], 0.5\n",
    "\n",
    "  # perform the training with given hyper-parameters:\n",
    "  tflr.train(X, Yoh_, 1000)\n",
    "\n",
    "  # predict probabilities of the data points\n",
    "  probs = tflr.eval(X)\n",
    "\n",
    "  # print performance (per-class precision and recall)\n",
    "\n",
    "  # draw results, decision surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignments:\n",
    "\n",
    "- Complete the implementation of the class `TFLogreg` and check whether your program achieves the same results as the corresponding programs from lab 0.\n",
    "- Add regularization in a way that you form the loss as a sum of cross entropy and the L2 norm of the vectorized weight matrix multiplied with a hyperparameter `param_lambda`. Test the effect of regularization to the shape of the decision surface.\n",
    "- Experiment with different values of hyperparameters. Find combinations of hyperparameters for which your program is unable to find a satisfcatory solution and try to explain what is happening. \n",
    "\n",
    "When you are satisfied with the execution results, save the code in the file `tf_logreg.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Configurable deep models in Tensorflow\n",
    "\n",
    "Our next task is to expand the Tensorflow implementation of logistic regression in a way to enable simple creation of configurable fully connected classification models. Your solution should be encapsulated in the class `TFDeep` which should have the same interface as the class `TFLogreg`. The `TFDeep` constructor receives the configuration hyperparameter in the form of a list of layer dimensionalities. The element at index 0 defines the dimensionality of the data. Elements at indices `1` to `n-2` (if any) define the number of activations in hidden layers. The last number in the configuration list (the element at index `n-1`) corresponds to the number of classes (the model is supposed to perform the classification).\n",
    "\n",
    "For example, a configuration `[2,3]` gives rise to multi-class logistic regression of two dimensional data into three classes. The configuration `[2,5,2]` specifies a model with one hidden layer `h` which contains `5` activations:\n",
    "\n",
    "```python\n",
    "h = f (X * W_1 + b_1)\n",
    "probs = softmax(h * W_2 + b_2) \n",
    "```\n",
    "\n",
    "In this example, the dimensions of graph nodes should be as follows (question marks stand for the unknown cardinality of the data set we apply the model to):\n",
    "\n",
    "```\n",
    "  X     ... [?,2]\n",
    "  W_1   ... [2,5]\n",
    "  b_1   ... [1,5]\n",
    "  h_1   ... [?,5]\n",
    "  W_2   ... [5,3]\n",
    "  b_2   ... [1,3]\n",
    "  probs ... [?,3]\n",
    "```\n",
    "\n",
    "Implementation of the class `TFDeep` should be very similar to what we had for the `TFLogreg` class. Most of the work will be in the constructor because now the graph construction depends on the configuration hyperparameter. Since the number of hidden layers may differ, the weight matrices and bias vectors should be placed in lists (let's call them `self.W` and `self.b`). The same goes for matrices of hidden layers which can be stored in the list `self.h`. Parameter dimensions can be defined within a loop through the configuration list. For better readability of the source code, we advise to factor the formulation of the model into a separate function. Hidden layer nonlinearities can be defined using TensorFlow's functions `tf.ReLU`, `tf.sigmoid` or `tf.tanh`.\n",
    "\n",
    "Assignments:\n",
    "\n",
    "- Implement the class `TFDeep` and try out configuration `[2,3]` on the same data as in the former assignment (the test code should be very similar). Make sure the results are the same as before.\n",
    "- Make sure that all model parameters are given suitable symbolic names (if necessary, help yourself with the [documentation](https://www.tensorflow.org/api_docs/python/tf/Variable)). Write a method `count_params` which prints out symbolic names of all trainable parameters by traversing the return value of the function `tf.trainable_variables`. Print the total number of model parameters (e.g. for configuration `[2,3]` the result should be `9`).\n",
    "- Try out your code on data returned by calling `data.sample_gmm_2d(4, 2, 40)` and `data.sample_gmm_2d(6, 2, 10)`, using configurations `[2,2]`, `[2,10,2]` and `[2,10,10,2]`. Print accuracy, recall, precision and average precision. Display the classification results and observe the decision surface. If there is no convergence, consider changing the hyperparameters.\n",
    "- Compare the results obtained with the hidden activation function set to `ReLU` and the sigmoid function. For small problems, sigmoid may achieve better results than `ReLU` due to continuity. The main advantage of the `ReLU` is that it doesn't saturate, so there are no vanishing gradients in deeper models. \n",
    "\n",
    "Based on parameters and the state of the random number generator, your result could be similar to the animation below (our hyperparameters were: `K=6`, `C=2`, `N=10`, `param_niter=1e4`, `param_delta=0.1`, `param_lambda=1e-4` (regularization coefficient), `config=[2,10,10,2]`, `ReLU`).\n",
    "\n",
    "-- image\n",
    "\n",
    "When you are satisfied with the execution results, save the code in the `file tf_logreg.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Comparison to a kernel SVM\n",
    "\n",
    "Recall the properties of the kernel SVM (model, loss, optimization) and read the documentation od the module `svm` from the library `scikit-learn`. Design the class KSVMWrap as a thin wrapper around the module `sklearn.svm` which you are going to be apply to the same two-dimensional data as before. Considering the spmlicity of our wrapper, the training can be done from the constructor, while class predictions, classification scores (required for average precision) and support vectors can be fetched in methods. Make the interface of the class as follows:\n",
    "\n",
    "```python\n",
    "Metode:\n",
    "  __init__(self, X, Y_, param_svm_c=1, param_svm_gamma='auto'):\n",
    "    Constructs the wrapper and trains the RBF SVM classifier\n",
    "    X,Y_:            data and indices of correct data classes\n",
    "    param_svm_c:     relative contribution of the data cost\n",
    "    param_svm_gamma:  RBF kernel width\n",
    "\n",
    "  predict(self, X):\n",
    "    Predicts and returns the class indices of data X\n",
    "\n",
    "  get_scores(self, X):\n",
    "    Returns the classification scores of the data\n",
    "    (you will need this to calculate average precision).\n",
    "\n",
    "  suport:\n",
    "    Indices of data chosen as support vectors\n",
    "```\n",
    "\n",
    "Assignments:\n",
    "\n",
    "- Modify the function `data.graph.data` by introducing the argument `special`. The argument `special` assigns a list of data indices which are to be emphasized by doubling the size of their symbols.\n",
    "- Test your class on the data from two classes sampled from mixtures of Gaussian distributions. As usual, display the standard performance metrics (accuracy, recall, precision, average precision).\n",
    "- Compare the model performance implemented by classes `TFDeep` and `KSVMWrap` on a larger number of random data sets. What are the pros and cons of their loss functions? Which of the two guarantees a better performance? Which of the two can take a larger number of parameters? Which of the two would be more suitable for 2D data sampled from Gaussian mixtures?\n",
    "- Plot the decision surface and classification results of RBF SVM. Make us of the argument `special` of the function `data.graph.data` to emphasize the display of support vectors. Based on the parameters and the random number generator, your results could resemble the following animation. (our hyperparameters were: `K=6`, `C=2`, `N=10`, `param_svm_c=1`, `param_svm_gamma='auto'`). \n",
    "\n",
    "-- image\n",
    "\n",
    "When you are satisfied with the execution results, save the code in the file `tf_logreg.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Case study: MNIST\n",
    "\n",
    "So far, the trained models haven't been evaluated on a separate test set. Such experiments can not provide an estimate of the generalization performance. They are therefore appropriate only in early experiments where we test whether a model has enough capacity for the given task.\n",
    "\n",
    "In this exercise we shall explore generalization performance on real data. The MNIST dataset is a collection of hand-written images of digits from 0 to 9. Each digit is represented by an image of 28x28 pixels. MNIST contains 50000 training images and 10000 testing images. The dataset can be (down)loaded with the following code:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.app.flags.DEFINE_string('data_dir', \n",
    "  '/tmp/data/', 'Directory for storing data')\n",
    "mnist = input_data.read_data_sets(\n",
    "  tf.app.flags.FLAGS.data_dir, one_hot=True)\n",
    "```\n",
    "\n",
    "Now, the sets of images and class indices are represented by numpy matrices `train.images`, `train.labels`, `test.images` i `test.labels` which are available as attributes of the mnist object. We can find the dimension of the data by querying the shape of those matrices.\n",
    "\n",
    "```\n",
    "N=mnist.train.images.shape[0]\n",
    "D=mnist.train.images.shape[1]\n",
    "C=mnist.train.labels.shape[1]\n",
    "```\n",
    "\n",
    "As before, the datapoints are stored as rows of the data matrix. If we want to display them, we must reshape them to the original resolution and pass them to the plt.imshow function with arguments `cmap = plt.get_cmap('gray')`, `vmin = 0` to `vmax = 1.`\n",
    "\n",
    "Assignments:\n",
    "\n",
    "- Train a `TFDeep` model with configuration `[784,10]` on MNIST. Plot and comment the weight matrices for each digit separately\n",
    "- Train the models with configurations `[784,10]`, `[784,100,10]`, `[784,100,100,10]` and `[784,100,100,100,10]` and compare their performance and confusion matrices on train and test sets. If you don't have a functioning GPU, you don't need to run experiments on the last two configurations. Pay attention that deeper models require more iterations and a smaller learning rate. For the most successfull model, plot images which contribute most to the loss function.\n",
    "- Study the effect of regularization to the performance of deep models on the train and the test subsets.\n",
    "- Randomly separate 1/5 of the training data into the validation set. Evaluate validation performance after each epoch and return the model with the best validation performance (early stopping). Quantify the effect of this procedure to the final loss and to the generalization.\n",
    "- Implement stohastic gradient descent with training on mini-batches. At the beginning of each epoch, shuffle the data and partition it into `sz_batch` mini-batches. Then perform one training step for each mini-batch. Store the code into the `train_mb` method of the class `TFDeep`. Estimate the effect of the convergence quality and achieved performance for the most successful configuration of the previous assignment.\n",
    "- Change the optimizer to `tf.train.AdamOptimizer` with fixed learning rate of `1e-4`. Estimate the effect of this change to the quality of convergene and achieved performance.\n",
    "- Try ADAM with a variable learning rate. Make use of the function [tf.train.exponential_decay](https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay). Leave the initial learning rate as before, and change the other hyperparameters to `decay_rate=1-1e-4` i `decay_stepsi=1`.\n",
    "- Train a linear and a kernel SVM classifier using the module `sklearn.svm`. Use the one vs one SVM variant to allow the classification of multiclass data. The experiment might require some patience since the training and evaluation may takes up to more than half an hour. Compare the achieved performance with the performance of deep models. \n",
    "\n",
    "When you are satisfied with the execution results, save the code in the file `mnist_shootout.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Batch normalization (extra credit)\n",
    "\n",
    "Study the batch normalization [technique](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization) for fully connected models. Expand the deep classifier by fitting a batchnorm layer after the affine transformations in each hidden layer. Be careful to allow the change of batch-normalization parameters [only](https://www.alexirpan.com/2017/04/26/perils-batch-norm.html) during [training](http://r2rt.com/implementing-batch-normalization-in-tensorflow.html). Compare the performance to what you have obtained with the basic deep model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
